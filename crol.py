from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import tqdm
from itertools import combinations
import pandas as pd

class RecursiveDOMExplorer:
    def __init__(self, root_node):
        """
        :param root_node: ì¬ê·€ íƒìƒ‰ì˜ ì‹œì‘ ë…¸ë“œ (BeautifulSoup Tag)
        """
        self.root = root_node

    def traverse(self, node=None, depth=0):
        """
        ì§€ì •í•œ ë…¸ë“œë¶€í„° ì‹œì‘í•˜ì—¬ ìì‹ ë…¸ë“œë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.
        ê° ë…¸ë“œì˜ íƒœê·¸ ì´ë¦„ê³¼ ì†ì„±, í…ìŠ¤íŠ¸(ìˆëŠ” ê²½ìš°)ë¥¼ ë“¤ì—¬ì“°ê¸°ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
        :param node: íƒìƒ‰í•  í˜„ì¬ ë…¸ë“œ (ì—†ìœ¼ë©´ self.root ì‚¬ìš©)
        :param depth: ë“¤ì—¬ì“°ê¸° ê¹Šì´ (ì¬ê·€ í˜¸ì¶œì‹œ ì‚¬ìš©)
        """
        if node is None:
            node = self.root

        indent = " " * (depth * 2)  # depthì— ë”°ë¼ ë“¤ì—¬ì“°ê¸°

        # Tag ê°ì²´ì¸ ê²½ìš° (ì˜ˆ: <div>, <p> ë“±)
        if hasattr(node, 'name') and node.name is not None:
            # ë…¸ë“œ ì´ë¦„ê³¼ ì†ì„± ì¶œë ¥
            print(f"{indent}Tag: {node.name}  Attributes: {node.attrs}")
        else:
            # NavigableString ë“±ì˜ ê²½ìš°, í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥ (ë¹ˆ ë¬¸ìì—´ì€ ê±´ë„ˆëœ€)
            text = str(node).strip()
            if text:
                print(f"{indent}Text: {text}")

        # í˜„ì¬ ë…¸ë“œì˜ ìì‹ë“¤(contents)ì„ ìˆœíšŒí•˜ë©´ì„œ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
        if hasattr(node, 'contents'):
            for child in node.contents:
                self.traverse(child, depth + 1)
def jaccard_similarity(text1, text2):
    """ ë‘ ê°œì˜ ë¬¸ì¥ ê°„ Jaccard Similarity ê³„ì‚° """
    set1, set2 = set(text1.split()), set(text2.split())
    intersection = len(set1 & set2)  # ê³µí†µ ë‹¨ì–´ ê°œìˆ˜
    union = len(set1 | set2)  # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜
    return intersection / union if union != 0 else 0
class TapName:
    def __init__(self, driver):
        self.opinion = "productOpinion"
        self.companyReview = "companyReview"
        self.mcReview = "mcReview"
        self.front = "danawa-prodBlog-"
        self.mid = "-button-tab-"
        self.driver = driver
        self.review=[]
        #review Parsing parm name
        self.second_selector = 'div.common_paginate'
        self.third_selector = 'div.nums_area'
    def getfirst(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
        tab.click()
        result=self.extract_reviews_general(soup)
        return result
    def get_second(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        result=self.extract_reviews_general(soup)
        return result
    def oneshot(self):
        self.review.append(self.getfirst())
        self.review.extend(self.get_second())
        return self.review
    def clean(self):
        self.driver.quit()
        return self.review
    def oneshot_iter(self):
        out=[]
        out.extend(self.oneshot())
        try:
            flag=True
            n=2
            while flag:
                flag=self.click_opinion_page(n)
                if flag:
                    out.extend(self.get_second())
                    n=n+1
                else:
                    print("endnum :{}",n)
        except:
            print("fail resource release")
            return False
        return out    
    @staticmethod 
    def extract_reviews_general(soup, similarity_threshold=0.7):
        reviews = []
        candidate_selectors = [
            'p.danawa-prodBlog-productOpinion-list-self',
            ("div",{"id":"danawa-prodBlog-productOpinion-list-self"}),
            ('div', {"id": "danawa-prodBlog-companyReview-content-list"}),
        ]     
        for selector in candidate_selectors:
            if isinstance(selector, str):
                elems = soup.select(selector)
            else:
                elems = soup.find_all(*selector)  
            if elems:  
                filtered_elems = []
                for elem in elems:
                    parent_div = elem.find_all('li', class_="danawa-prodBlog-companyReview-clazz-more")
                    if parent_div:  
                        filtered_elems.extend(parent_div)
                    else:
                        parent_div = elem.find_all('p', class_="danawa-prodBlog-productOpinion-clazz-content", attrs={"data-seq": "N"})
                        if parent_div:  
                            filtered_elems.extend(parent_div)
        for felem in filtered_elems:
            text=felem.find_all('div', class_="atc")
            if text:
                for i in text:
                    text=" ".join(set(i.get_text(separator=' ', strip=True).split()))
                    if text:
                        reviews.append(text)
            else:
                text=" ".join(set(felem.get_text(separator=' ', strip=True).split()))
                if text:
                    reviews.append(text)
        # âœ… **ìœ ì‚¬ë„ í•„í„°ë§ (Jaccard Similarity ì´ìš©)**
        unique_reviews = []
        for i, review in enumerate(reviews):
            is_duplicate = False
            for unique_review in unique_reviews:
                if jaccard_similarity(review, unique_review) > similarity_threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_reviews.append(review)
        return unique_reviews
    def click_opinion_page(self, page):
        try:
            # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (ìˆ˜ì •ë¨: CSS Selectorë¡œ ë³€ê²½)
            first = self.driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-companyReview-content-list")
            if not first:
                return False
            # âœ… ë‘ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.second_selector ì‚¬ìš©)
            Spagr = self.driver.find_elements(By.CSS_SELECTOR, self.second_selector)
            if not Spagr:
                return False
            # âœ… ì„¸ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.third_selector ì‚¬ìš©)
            Tpagr = self.driver.find_elements(By.CSS_SELECTOR, self.third_selector)
            if not Tpagr:
                return False
            # âœ… í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (WebDriverWait ì ìš©)
            page_buttons = WebDriverWait(self.driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.page_num[data-pagenumber]"))
            )
            # âœ… íŠ¹ì • í˜ì´ì§€ ë²ˆí˜¸ í´ë¦­ (ë¬¸ìì—´ ë³€í™˜ ì¶”ê°€)
            for button in page_buttons:
                page_number = button.get_attribute("data-pagenumber")
                if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                    self.driver.execute_script("arguments[0].click();", button)
                    return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜    
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
            return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜
        return False  # í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ì„ ê²½ìš° False
 
def detail (url,trynum=5):
    options = Options()
    options.add_argument('--headless') 
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)
    # í¬ë¡¤ë§í•  í˜ì´ì§€ URL (ì‹¤ì œ URLë¡œ ë³€ê²½)
    driver.get("https://prod.danawa.com/"+url)
    # ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•´ ì¶©ë¶„í•œ ì‹œê°„ ëŒ€ê¸° (WebDriverWait ì‚¬ìš© ê¶Œì¥)
    time.sleep(5)
    # ë‘ ê°œì˜ ì˜ê²¬/ë¦¬ë·° íƒ­ ID ëª©ë¡ (í•„ìš”í•œ ê²½ìš° ë‘ íƒ­ ëª¨ë‘ ì˜ê²¬/ë¦¬ë·° ë²”ì£¼ì— í•´ë‹¹í•œë‹¤ê³  ê°€ì •)
    desired_tab_ids = {
        "danawa-prodBlog-productOpinion-button-tab-productOpinion",
        "danawa-prodBlog-productOpinion-button-tab-companyReview"
    }
    # í˜„ì¬ í™œì„± íƒ­ì˜ <a> íƒœê·¸ ì„ íƒ (ìƒìœ„ íƒ­ ì˜ì—­ì—ì„œ í™œì„± íƒ­ì€ li ìš”ì†Œì— "on" í´ë˜ìŠ¤ê°€ ë¶™ì–´ ìˆìŠµë‹ˆë‹¤)
    active_tab_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "li.tab_item.on > a"))
    )
    active_tab_id = active_tab_element.get_attribute("id")
    if active_tab_id not in desired_tab_ids:
        print("í™œì„± íƒ­ì´ ì˜ê²¬/ë¦¬ë·°ê°€ ì•„ë‹ˆë¯€ë¡œ ì˜ê²¬/ë¦¬ë·° íƒ­ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ìœ¼ë¡œ "ë‹¤ë‚˜ì™€ ìƒí’ˆì˜ê²¬" íƒ­ì„ ì„ íƒí•©ë‹ˆë‹¤.
        T=0
        while T<trynum:
            try:
                product_opinion_tab = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.ID, "danawa-prodBlog-productOpinion-button-tab-productOpinion"))
                )
                product_opinion_tab.click(),
            except:
                T=T+1
                print("retry")
        if T==trynum:
            print("fail")
            return False
    # ì›í•˜ëŠ” ì‹œì‘ ë…¸ë“œë¥¼ ì§€ì •í•œ í›„, íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    html = driver.page_source  # Seleniumì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ HTML
    soup = BeautifulSoup(html, 'html.parser')
    try:
        reviews=TapName(driver)
        out=reviews.oneshot_iter()
        driver.quit()
        return out
    except:
        print("fail resource release")
        return reviews.clean()
def click_page(page,driver,timeout=10):
    try:
        # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (í˜ì´ì§€ ì½˜í…ì¸ ê°€ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#productListArea"))
        )
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.number_wrap"))
        )
        page_buttons = driver.find_elements(By.CSS_SELECTOR, "div.number_wrap a.num")
        for button in page_buttons:
            page_number = button.text.strip()
            if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                driver.execute_script("arguments[0].click();", button)
                WebDriverWait(driver, timeout).until(EC.staleness_of(button))
                return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜ 
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
        return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜


    
 

def get_data_from_url(url):
    # Selenium ì˜µì…˜ ì„¤ì • (headless ëª¨ë“œ)
    options = Options()
    options.add_argument('--headless')       # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    # ChromeDriver ì‹¤í–‰ (ê²½ë¡œëŠ” í™˜ê²½ë³€ìˆ˜ì— ìˆê±°ë‚˜ ì§ì ‘ ì§€ì •)
    driver = webdriver.Chrome(options=options)
    # í¬ë¡¤ë§í•  URL (ì‹¤ì œ ë‹¤ë‚˜ì™€ ìƒí’ˆ ëª©ë¡ URLë¡œ ë³€ê²½)   
    driver.get(url)
    # ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•´ ëŒ€ê¸° (WebDriverWait ì‚¬ìš© ê¶Œì¥)
    time.sleep(5)
    # ë Œë”ë§ëœ HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
    product_lists=[]
    mainFlag=True
    loopnum=2  
    while mainFlag:
        html = driver.page_source
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'html.parser')
        # ì œí’ˆ ì •ë³´ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì—¬ëŸ¬ ìƒí’ˆì´ ìˆì„ ê²½ìš° ë°˜ë³µë¬¸ ì‚¬ìš©)
        all_product_divs = soup.find_all('div', class_='prod_info')
        # ì—¬ëŸ¬ ìƒí’ˆ ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        product_list = []
        for prod_info_div in all_product_divs:
            product_info = {}
            if prod_info_div:
                # ì œí’ˆëª… ì¶”ì¶œ: <p class="prod_name"> ë‚´ë¶€ì˜ <a name="productName"> íƒœê·¸
                prod_name_tag = prod_info_div.find('p', class_='prod_name')
                if prod_name_tag:
                    a_tag = prod_name_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = a_tag.get_text(strip=True)
                prod_op_link_tag = prod_info_div.find('p', class_='prod_name')
                if prod_op_link_tag:
                    op_link_tag = prod_op_link_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = op_link_tag.get_text(strip=True)
                comment_div = prod_info_div.find('div', class_='meta_item mt_comment')
                if comment_div:
                    a_comment = comment_div.find('a')
                    if a_comment:
                        # ì œí’ˆì˜ê²¬ ê´€ë ¨ ì •ë³´: ë§í¬, ì˜ê²¬ ìˆ˜ ë“±
                        product_info['opinion'] = {} 
                        product_info['opinion']['link'] = a_comment.get('href')
                        strong_tag = a_comment.find('strong')
                        if strong_tag:
                            product_info['opinion']['count'] = strong_tag.get_text(strip=True)
                # ìŠ¤í™ ì˜ì—­ ì¶”ì¶œ: <div class="spec_list"> ë‚´ë¶€ì˜ ëª¨ë“  ìŠ¤í™ ì •ë³´ ì¶”ì¶œ
                spec_items = []
                spec_div = prod_info_div.find('div', class_='spec_list')
                if spec_div:
                    # <a class="view_dic"> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ì¶”ì¶œ
                    for a in spec_div.find_all('a', class_='view_dic'):
                        text = a.get_text(strip=True)
                        spec_items.append(text)
                # ì¶”ì¶œí•œ ìŠ¤í™ ë°°ì—´ì„ ì œí’ˆ ì •ë³´ì— ì¶”ê°€ (ë°°ì—´ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê±°ë‚˜, join()ìœ¼ë¡œ ë¬¸ìì—´ ê²°í•© ê°€ëŠ¥)
                product_info['specs'] = spec_items
                product_list.append(product_info)
                
        #for i in tqdm.tqdm(range(0, len(product_list))):
        #    product_list[i]['opinion']["reviews"] =detail(product_list[i]['opinion']['link'])
        product_lists.append(product_list)
        mainFlag=click_page(loopnum,driver)
        loopnum=loopnum+1

    driver.quit()
    return product_lists

def extract_name(data,fname="danawa.csv"):
    out=[]
    for d in data:
        for i in d:
            out.append(i["name"])
    out=pd.DataFrame(out)
    out.to_csv(fname)
    return out
if __name__ == "__main__":
    url = 'https://prod.danawa.com/list/?cate=22254632s'
    data = get_data_from_url(url)
    extract_name(data)



