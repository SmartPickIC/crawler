import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import tqdm
from itertools import combinations
import pandas as pd
import h5py
import json
from queue import Queue

class RecursiveDOMExplorer:
    def __init__(self, root_node):
        """
        :param root_node: ì¬ê·€ íƒìƒ‰ì˜ ì‹œì‘ ë…¸ë“œ (BeautifulSoup Tag)
        """
        self.root = root_node

    def traverse(self, node=None, depth=0):
        """
        ì§€ì •í•œ ë…¸ë“œë¶€í„° ì‹œì‘í•˜ì—¬ ìì‹ ë…¸ë“œë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.
        ê° ë…¸ë“œì˜ íƒœê·¸ ì´ë¦„ê³¼ ì†ì„±, í…ìŠ¤íŠ¸(ìˆëŠ” ê²½ìš°)ë¥¼ ë“¤ì—¬ì“°ê¸°ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
        :param node: íƒìƒ‰í•  í˜„ì¬ ë…¸ë“œ (ì—†ìœ¼ë©´ self.root ì‚¬ìš©)
        :param depth: ë“¤ì—¬ì“°ê¸° ê¹Šì´ (ì¬ê·€ í˜¸ì¶œì‹œ ì‚¬ìš©)
        """
        if node is None:
            node = self.root

        indent = " " * (depth * 2)  # depthì— ë”°ë¼ ë“¤ì—¬ì“°ê¸°

        # Tag ê°ì²´ì¸ ê²½ìš° (ì˜ˆ: <div>, <p> ë“±)
        if hasattr(node, 'name') and node.name is not None:
            # ë…¸ë“œ ì´ë¦„ê³¼ ì†ì„± ì¶œë ¥
            print(f"{indent}Tag: {node.name}  Attributes: {node.attrs}")
        else:
            # NavigableString ë“±ì˜ ê²½ìš°, í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥ (ë¹ˆ ë¬¸ìì—´ì€ ê±´ë„ˆëœ€)
            text = str(node).strip()
            if text:
                print(f"{indent}Text: {text}")

        # í˜„ì¬ ë…¸ë“œì˜ ìì‹ë“¤(contents)ì„ ìˆœíšŒí•˜ë©´ì„œ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
        if hasattr(node, 'contents'):
            for child in node.contents:
                self.traverse(child, depth + 1)
def jaccard_similarity(text1, text2):
    """ ë‘ ê°œì˜ ë¬¸ì¥ ê°„ Jaccard Similarity ê³„ì‚° """
    set1, set2 = set(text1.split()), set(text2.split())
    intersection = len(set1 & set2)  # ê³µí†µ ë‹¨ì–´ ê°œìˆ˜
    union = len(set1 | set2)  # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜
    return intersection / union if union != 0 else 0
class TapName:
    def __init__(self, driver):
        self.opinion = "productOpinion"
        self.companyReview = "companyReview"
        self.mcReview = "mcReview"
        self.front = "danawa-prodBlog-"
        self.mid = "-button-tab-"
        self.driver = driver
        self.review=[]
        #review Parsing parm name
        self.second_selector = 'div.common_paginate'
        self.third_selector = 'div.nums_area'
    def getfirst(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
        tab.click()
        result=self.extract_reviews_general(soup)
        return result
    def get_second(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        result=self.extract_reviews_general(soup)
        return result
    def oneshot(self):
        self.review.append(self.getfirst())
        self.review.extend(self.get_second())
        return self.review
    def clean(self):
        self.driver.quit()
        return self.review
    def oneshot_iter(self):
        out=[]
        try:
            out.extend(self.oneshot())
        except:
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
            tab.click()
        try:
            flag=True
            n=2
            while flag:
                flag=self.click_opinion_page(n)
                if flag:
                    out.extend(self.get_second())
                    n=n+1                   
        except:
            if out:
                return out
            else:
                return False
        return out    
    @staticmethod 
    def extract_reviews_general(soup, similarity_threshold=0.7):
        reviews = []
        candidate_selectors = [
            'p.danawa-prodBlog-productOpinion-list-self',
            ("div",{"id":"danawa-prodBlog-productOpinion-list-self"}),
            ('div', {"id": "danawa-prodBlog-companyReview-content-list"}),
        ]     
        for selector in candidate_selectors:
            if isinstance(selector, str):
                elems = soup.select(selector)
            else:
                elems = soup.find_all(*selector)  
            if elems:  
                filtered_elems = []
                for elem in elems:
                    parent_div = elem.find_all('li', class_="danawa-prodBlog-companyReview-clazz-more")
                    if parent_div:  
                        filtered_elems.extend(parent_div)
                    else:
                        parent_div = elem.find_all('p', class_="danawa-prodBlog-productOpinion-clazz-content", attrs={"data-seq": "N"})
                        if parent_div:  
                            filtered_elems.extend(parent_div)
        for felem in filtered_elems:
            text=felem.find_all('div', class_="atc")
            if text:
                for i in text:
                    text=" ".join(set(i.get_text(separator=' ', strip=True).split()))
                    if text:
                        reviews.append(text)
            else:
                text=" ".join(set(felem.get_text(separator=' ', strip=True).split()))
                if text:
                    reviews.append(text)
        # âœ… **ìœ ì‚¬ë„ í•„í„°ë§ (Jaccard Similarity ì´ìš©)**
        unique_reviews = []
        for i, review in enumerate(reviews):
            is_duplicate = False
            for unique_review in unique_reviews:
                if jaccard_similarity(review, unique_review) > similarity_threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_reviews.append(review)
        return unique_reviews
    def click_opinion_page(self, page):
        try:
            # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (ìˆ˜ì •ë¨: CSS Selectorë¡œ ë³€ê²½)
            first = self.driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-companyReview-content-list")
            if not first:
                return False
            # âœ… ë‘ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.second_selector ì‚¬ìš©)
            Spagr = self.driver.find_elements(By.CSS_SELECTOR, self.second_selector)
            if not Spagr:
                return False
            # âœ… ì„¸ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.third_selector ì‚¬ìš©)
            Tpagr = self.driver.find_elements(By.CSS_SELECTOR, self.third_selector)
            if not Tpagr:
                return False
            # âœ… í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (WebDriverWait ì ìš©)
            page_buttons = WebDriverWait(self.driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.page_num[data-pagenumber]"))
            )
            # âœ… íŠ¹ì • í˜ì´ì§€ ë²ˆí˜¸ í´ë¦­ (ë¬¸ìì—´ ë³€í™˜ ì¶”ê°€)
            for button in page_buttons:
                page_number = button.get_attribute("data-pagenumber")
                if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                    self.driver.execute_script("arguments[0].click();", button)
                    return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜    
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
            return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜
        return False  # í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ì„ ê²½ìš° False
 
def detail (url,trynum=5):
    options = Options()
    options.add_argument('--headless') 
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(service = Service(),options=options)
    # í¬ë¡¤ë§í•  í˜ì´ì§€ URL (ì‹¤ì œ URLë¡œ ë³€ê²½)
    driver.get("https://prod.danawa.com/"+url)
    # ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•´ ì¶©ë¶„í•œ ì‹œê°„ ëŒ€ê¸° (WebDriverWait ì‚¬ìš© ê¶Œì¥)
    WebDriverWait(driver, 5).until(lambda driver: driver.execute_script("return document.readyState") == "complete")
    # ë‘ ê°œì˜ ì˜ê²¬/ë¦¬ë·° íƒ­ ID ëª©ë¡ (í•„ìš”í•œ ê²½ìš° ë‘ íƒ­ ëª¨ë‘ ì˜ê²¬/ë¦¬ë·° ë²”ì£¼ì— í•´ë‹¹í•œë‹¤ê³  ê°€ì •)
    desired_tab_ids = {
        "danawa-prodBlog-productOpinion-button-tab-productOpinion",
        "danawa-prodBlog-productOpinion-button-tab-companyReview"
    }
    # í˜„ì¬ í™œì„± íƒ­ì˜ <a> íƒœê·¸ ì„ íƒ (ìƒìœ„ íƒ­ ì˜ì—­ì—ì„œ í™œì„± íƒ­ì€ li ìš”ì†Œì— "on" í´ë˜ìŠ¤ê°€ ë¶™ì–´ ìˆìŠµë‹ˆë‹¤)
    active_tab_elements = WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.tab_item"))
    )
    #driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-productOpinion-button-tab-productOpinion")

    for active_tab_element in active_tab_elements:
        classname = active_tab_element.get_attribute("class")
        if classname.endswith("on"):
            if not 'bookmark_cm_opinion_item'==active_tab_element.get_attribute("id"):
                        T=0
                        while T<trynum:
                            try:
                                product_opinion_tab = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "#danawa-prodBlog-productOpinion-button-tab-productOpinion")))
                                product_opinion_tab.click(),
                                break
                            except:
                                T=T+1
                                print("retry")
                        if T==trynum:
                            print("fail")
                            return False
            else:
                break
    # ì›í•˜ëŠ” ì‹œì‘ ë…¸ë“œë¥¼ ì§€ì •í•œ í›„, íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    html = driver.page_source  # Seleniumì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ HTML
    soup = BeautifulSoup(html, 'html.parser')
    try:
        reviews=TapName(driver)
        out=reviews.oneshot_iter()
        driver.quit()
        return out
    except:
        print("fail resource release")
        return reviews.clean()
def click_page(page,driver,timeout=10):
    try:
        # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (í˜ì´ì§€ ì½˜í…ì¸ ê°€ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#productListArea"))
        )
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.number_wrap"))
        )
        page_buttons = driver.find_elements(By.CSS_SELECTOR, "div.number_wrap a.num")
        for button in page_buttons:
            page_number = button.text.strip()
            if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                driver.execute_script("arguments[0].click();", button)
                WebDriverWait(driver, timeout).until(EC.staleness_of(button))
                return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜ 
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
        return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜
def get_data_from_url_multi_thread(url,start,end,num_threads=8):
    product_lists = {}
    task_queue = Queue()

    # âœ… í¬ë¡¤ë§í•  í˜ì´ì§€ë¥¼ íì— ì‚½ì…
    for page in range(start, end + 1):
        task_queue.put(page)

    
    with concurrent.futures.ThreadPoolExecutor(num_threads) as executor:
        futures = {}

        # âœ… ì´ˆê¸° ìŠ¤ë ˆë“œ ì‹¤í–‰ (ìµœëŒ€ `num_threads` ê°œìˆ˜ë§Œí¼)
        for _ in range(min(num_threads, task_queue.qsize())):
            page = task_queue.get()
            future = executor.submit(get_data_from_url_single, url, page)
            futures[future] = page

        # âœ… ë™ì ìœ¼ë¡œ ì‘ì—… í• ë‹¹
        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=end - start + 1):
            page = futures[future]
            try:
                #print(f"DEBUG: future ê°ì²´ = {future}")  # âœ… futureê°€ ë­”ì§€ í™•ì¸
                data = future.result()  # âŒ ì—¬ê¸°ì„œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
                #print(f"DEBUG: future.result() = {data}")  # âœ… ì—¬ê¸°ê¹Œì§€ ë„ë‹¬í•˜ë©´ ì •ìƒ ë°˜í™˜ë¨
                product_lists[page] = data
            except Exception as e:
                #print(f"âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                data = ["get fail"]  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                product_lists[page] = data

           # âœ… í˜ì´ì§€ ìˆœì„œ ìœ ì§€í•˜ì—¬ ì €ì¥

            # âœ… ìƒˆë¡œìš´ ì‘ì—… í• ë‹¹ (ìŠ¤ë ˆë“œê°€ í•˜ë‚˜ ëë‚˜ë©´ ìƒˆë¡œìš´ í˜ì´ì§€ í¬ë¡¤ë§)
            if not task_queue.empty():
                new_page = task_queue.get()
                new_future = executor.submit(get_data_from_url_single, url, new_page)
                futures[new_future] = new_page

            # âœ… ê¸°ì¡´ future ì œê±° (ë©”ëª¨ë¦¬ í•´ì œ)
            del futures[future]

    # âœ… í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì •ë¦¬
    return [product_lists[page] for page in range(start, end + 1)]




def get_data_from_url_single(url,num):
    # Selenium ì˜µì…˜ ì„¤ì • (headless ëª¨ë“œ)
    options = Options()
    options.add_argument('--headless')       # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    # ChromeDriver ì‹¤í–‰ (ê²½ë¡œëŠ” í™˜ê²½ë³€ìˆ˜ì— ìˆê±°ë‚˜ ì§ì ‘ ì§€ì •)
    driver = webdriver.Chrome(service = Service(),options=options)
    try:
        # âœ… í˜ì´ì§€ ì´ë™
        driver.get(url)
        
        # âœ… ì²« ë²ˆì§¸ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

        # âœ… í˜ì´ì§€ ì´ë™ (num í˜ì´ì§€ í´ë¦­)
        if num != 1:
            click_page(num, driver)  
            WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

        html = driver.page_source
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'html.parser')
        # ì œí’ˆ ì •ë³´ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì—¬ëŸ¬ ìƒí’ˆì´ ìˆì„ ê²½ìš° ë°˜ë³µë¬¸ ì‚¬ìš©)
        all_product_divs = soup.find_all('div', class_='prod_info')
        # ì—¬ëŸ¬ ìƒí’ˆ ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        product_list = []
        for prod_info_div in all_product_divs:
            product_info = {}
            if prod_info_div:
                # ì œí’ˆëª… ì¶”ì¶œ: <p class="prod_name"> ë‚´ë¶€ì˜ <a name="productName"> íƒœê·¸
                prod_name_tag = prod_info_div.find('p', class_='prod_name')
                if prod_name_tag:
                    a_tag = prod_name_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = a_tag.get_text(strip=True)
                prod_op_link_tag = prod_info_div.find('p', class_='prod_name')
                if prod_op_link_tag:
                    op_link_tag = prod_op_link_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = op_link_tag.get_text(strip=True)
                comment_div = prod_info_div.find('div', class_='meta_item mt_comment')
                if comment_div:
                    a_comment = comment_div.find('a')
                    if a_comment:
                        # ì œí’ˆì˜ê²¬ ê´€ë ¨ ì •ë³´: ë§í¬, ì˜ê²¬ ìˆ˜ ë“±
                        product_info['opinion'] = {} 
                        product_info['opinion']['link'] = a_comment.get('href')
                        strong_tag = a_comment.find('strong')
                        if strong_tag:
                            product_info['opinion']['count'] = strong_tag.get_text(strip=True)
                # ìŠ¤í™ ì˜ì—­ ì¶”ì¶œ: <div class="spec_list"> ë‚´ë¶€ì˜ ëª¨ë“  ìŠ¤í™ ì •ë³´ ì¶”ì¶œ
                spec_items = []
                spec_div = prod_info_div.find('div', class_='spec_list')
                if spec_div:
                    # <a class="view_dic"> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ì¶”ì¶œ
                    for a in spec_div.find_all('a', class_='view_dic'):
                        if a:
                            text = a.get_text(strip=True)
                            spec_items.append(text)
                # ì¶”ì¶œí•œ ìŠ¤í™ ë°°ì—´ì„ ì œí’ˆ ì •ë³´ì— ì¶”ê°€ (ë°°ì—´ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê±°ë‚˜, join()ìœ¼ë¡œ ë¬¸ìì—´ ê²°í•© ê°€ëŠ¥)
                product_info['specs'] = spec_items
                product_list.append(product_info)
               
        for i in range(0, len(product_list)):
            if 'opinion' in product_list[i]:
                try:
                    product_list[i]['opinion']["reviews"] =detail(product_list[i]['opinion']['link'])
                except:
                    product_list[i]['opinion']["reviews"] = ["no review"]
        return product_list  
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        product_list = ["get fail"]
        return product_list
    finally:
        driver.quit()  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¸Œë¼ìš°ì €ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ

        
 

def get_data_from_url(url):
    # Selenium ì˜µì…˜ ì„¤ì • (headless ëª¨ë“œ)
    options = Options()
    options.add_argument('--headless')       # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    # ChromeDriver ì‹¤í–‰ (ê²½ë¡œëŠ” í™˜ê²½ë³€ìˆ˜ì— ìˆê±°ë‚˜ ì§ì ‘ ì§€ì •)
    driver = webdriver.Chrome(service = Service(),options=options)
    # í¬ë¡¤ë§í•  URL (ì‹¤ì œ ë‹¤ë‚˜ì™€ ìƒí’ˆ ëª©ë¡ URLë¡œ ë³€ê²½)   
    driver.get(url)
    # ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•´ ëŒ€ê¸° (WebDriverWait ì‚¬ìš© ê¶Œì¥)
    WebDriverWait(driver, 5).until(lambda driver: driver.execute_script("return document.readyState") == "complete")

    # ë Œë”ë§ëœ HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
    product_lists=[]
    mainFlag=True
    loopnum=2  
    while mainFlag:
        html = driver.page_source
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'html.parser')
        # ì œí’ˆ ì •ë³´ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì—¬ëŸ¬ ìƒí’ˆì´ ìˆì„ ê²½ìš° ë°˜ë³µë¬¸ ì‚¬ìš©)
        all_product_divs = soup.find_all('div', class_='prod_info')
        # ì—¬ëŸ¬ ìƒí’ˆ ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        product_list = []
        for prod_info_div in all_product_divs:
            product_info = {}
            if prod_info_div:
                # ì œí’ˆëª… ì¶”ì¶œ: <p class="prod_name"> ë‚´ë¶€ì˜ <a name="productName"> íƒœê·¸
                prod_name_tag = prod_info_div.find('p', class_='prod_name')
                if prod_name_tag:
                    a_tag = prod_name_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = a_tag.get_text(strip=True)
                prod_op_link_tag = prod_info_div.find('p', class_='prod_name')
                if prod_op_link_tag:
                    op_link_tag = prod_op_link_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = op_link_tag.get_text(strip=True)
                comment_div = prod_info_div.find('div', class_='meta_item mt_comment')
                if comment_div:
                    a_comment = comment_div.find('a')
                    if a_comment:
                        # ì œí’ˆì˜ê²¬ ê´€ë ¨ ì •ë³´: ë§í¬, ì˜ê²¬ ìˆ˜ ë“±
                        product_info['opinion'] = {} 
                        product_info['opinion']['link'] = a_comment.get('href')
                        strong_tag = a_comment.find('strong')
                        if strong_tag:
                            product_info['opinion']['count'] = strong_tag.get_text(strip=True)
                # ìŠ¤í™ ì˜ì—­ ì¶”ì¶œ: <div class="spec_list"> ë‚´ë¶€ì˜ ëª¨ë“  ìŠ¤í™ ì •ë³´ ì¶”ì¶œ
                spec_items = []
                spec_div = prod_info_div.find('div', class_='spec_list')
                if spec_div:
                    # <a class="view_dic"> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ì¶”ì¶œ
                    for a in spec_div.find_all('a', class_='view_dic'):
                        if a:
                            text = a.get_text(strip=True)
                            spec_items.append(text)
                # ì¶”ì¶œí•œ ìŠ¤í™ ë°°ì—´ì„ ì œí’ˆ ì •ë³´ì— ì¶”ê°€ (ë°°ì—´ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê±°ë‚˜, join()ìœ¼ë¡œ ë¬¸ìì—´ ê²°í•© ê°€ëŠ¥)
                product_info['specs'] = spec_items
                product_list.append(product_info)        
        for i in tqdm.tqdm(range(0, len(product_list))):
            if 'opinion' in product_list[i]:
                product_list[i]['opinion']["reviews"] =detail(product_list[i]['opinion']['link'])
            
        product_lists.append(product_list)
        mainFlag=click_page(loopnum,driver)
        if loopnum>3:
            break
        loopnum=loopnum+1

    driver.quit()
    return product_lists
def save_hdf5(data, filename="danawa_data.h5"):
    with h5py.File(filename, "w") as f:
        json_data = json.dumps(data, ensure_ascii=False)  # âœ… JSON ì§ë ¬í™”
        f.create_dataset("danawa_reviews", data=json_data.encode("utf-8"))  # âœ… UTF-8 ì¸ì½”ë”©

def load_hdf5(filename="danawa_data.h5"):
    with h5py.File(filename, "r") as f:
        json_data = f["danawa_reviews"][()].decode("utf-8")  # âœ… UTF-8 ë””ì½”ë”©
        return json.loads(json_data)  # âœ… JSON íŒŒì‹± (ì›í˜• ë³µì›)

def save_hdf5_separate(data, filename="danawa_data_separate.h5"):
    with h5py.File(filename, "w") as f:
        for i, page in enumerate(data):
            page_data = json.dumps(page, ensure_ascii=False)  # JSON ë³€í™˜
            f.create_dataset(f"page_{i}", data=page_data.encode("utf-8"))  # UTF-8 ì¸ì½”ë”©

def load_hdf5_partial(filename="danawa_data_separate.h5", page_index=0):
    with h5py.File(filename, "r") as f:
        json_data = f[f"page_{page_index}"][()].decode("utf-8")  # íŠ¹ì • í˜ì´ì§€ë§Œ ë¡œë“œ
        return json.loads(json_data)  # JSON íŒŒì‹±

def extract_name(data,fname="danawa.csv"):
    out=[]
    for d in data:
        if not (d[0]=='get fail'):
            for i in d:
                out.append(i["name"])

    out=pd.DataFrame(out)
    out.to_csv(fname)
    return out
if __name__ == "__main__":
    url = 'https://prod.danawa.com/list/?cate=22254632s'
    #data = get_data_from_url(url)
    data = get_data_from_url_multi_thread(url,1,10)
    extract_name(data)
    save_hdf5(data, "danawa_data.h5")


