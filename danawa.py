import os
import re
from pathlib import Path
import tqdm
import pandas as pd
from pathlib import Path
import ast
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import tqdm
import pandas as pd
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from pathlib import Path
import pickle
import os
from pathlib import Path
from difflib import SequenceMatcher
import requests
import csv
import argparse
import hashlib
import threading
from time import sleep
import time
from typing import List, Any
import psutil
import sys
class ProductListManager:
    def __init__(self, 
                 backup_dir: str = 'backup',
                 memory_threshold_mb: int = 500,
                 max_items_before_dump: int = 1000,
                 backup_interval_seconds: int = 300,
                 max_total_size_gb: int = 20):
        """
        Initialize the ProductListManager.
        
        Args:
            backup_dir: Directory to store pickle backups
            memory_threshold_mb: Memory threshold in MB before forcing a dump
            max_items_before_dump: Maximum items to hold before forcing a dump
            backup_interval_seconds: Minimum time between backups in seconds
            max_total_size_gb: Maximum total size of all backups in GB
        """
        self.product_lists = []
        self.backup_dir = backup_dir
        self.memory_threshold = memory_threshold_mb * 1024 * 1024  # Convert to bytes
        self.max_items = 20  # ê¸°ë³¸ê°’ì„ 20ìœ¼ë¡œ ì„¤ì •
        self.max_total_size = max_total_size_gb * 1024 * 1024 * 1024  # GBë¥¼ bytesë¡œ ë³€í™˜
        self.backup_interval = backup_interval_seconds
        self.last_backup_time = time.time()
        self.backup_counter = 0
        
        # Create backup directory if it doesn't exist
        os.makedirs(backup_dir, exist_ok=True)
        
        # Load any existing backups
        self._load_latest_backup()

    def append(self, item: Any) -> None:
        """Add an item to the product list and handle automatic backup if needed."""
        self.product_lists.append(item)
        
        # Check if we need to dump based on various conditions
        if self._should_dump():
            self._dump_to_pickle()

    def _should_dump(self) -> bool:
        """Check if we should dump based on memory usage, item count, and time."""
        current_time = time.time()
        time_since_last_backup = current_time - self.last_backup_time
        
        # Get current memory usage
        process = psutil.Process(os.getpid())
        current_memory = process.memory_info().rss
        
        return (
            current_memory >= self.memory_threshold or
            len(self.product_lists) >= self.max_items or
            time_since_last_backup >= self.backup_interval
        )

    def _dump_to_pickle(self) -> None:
        """Dump current product lists to a pickle file and clear memory."""
        if not self.product_lists:
            return

        self.backup_counter += 1
        backup_filename = os.path.join(
            self.backup_dir, 
            f'product_lists_backup_{self.backup_counter}.pkl'
        )
        
        # Save to pickle file
        with open(backup_filename, 'wb') as f:
            pickle.dump(self.product_lists, f)
        
        # Clear the current list to free memory
        self.product_lists = []
        self.last_backup_time = time.time()

    def _load_latest_backup(self) -> None:
        """Load the most recent backup if it exists."""
        backup_files = sorted([
            f for f in os.listdir(self.backup_dir)
            if f.startswith('product_lists_backup_') and f.endswith('.pkl')
        ])
        
        if backup_files:
            latest_backup = backup_files[-1]
            self.backup_counter = int(latest_backup.split('_')[-1].split('.')[0])
            
            with open(os.path.join(self.backup_dir, latest_backup), 'rb') as f:
                self.product_lists = pickle.load(f)

    def get_all_data(self) -> List[Any]:
        """
        Combine all data from backups and current list.
        Warning: This may use a lot of memory if there are many backups.
        """
        all_data = []
        
        # Load data from all backup files
        backup_files = sorted([
            f for f in os.listdir(self.backup_dir)
            if f.startswith('product_lists_backup_') and f.endswith('.pkl')
        ])
        
        for backup_file in backup_files:
            with open(os.path.join(self.backup_dir, backup_file), 'rb') as f:
                all_data.extend(pickle.load(f))
        
        # Add current data
        all_data.extend(self.product_lists)
        
        return all_data

    def get_backup_info(self) -> List[dict]:
        """ë°±ì—… íŒŒì¼ë“¤ì˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        backup_files = sorted([
            f for f in os.listdir(self.backup_dir)
            if f.startswith('product_lists_backup_') and f.endswith('.pkl')
        ])
        
        info_list = []
        for backup_file in backup_files:
            file_path = os.path.join(self.backup_dir, backup_file)
            file_size = os.path.getsize(file_path)
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                info_list.append({
                    'filename': backup_file,
                    'size_mb': file_size / (1024 * 1024),
                    'items_count': len(data)
                })
        return info_list

    def load_specific_backup(self, backup_number: int) -> List[Any]:
        """íŠ¹ì • ë°±ì—… íŒŒì¼ë§Œ ë¡œë“œí•©ë‹ˆë‹¤."""
        filename = f'product_lists_backup_{backup_number}.pkl'
        file_path = os.path.join(self.backup_dir, filename)
        
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        else:
            raise FileNotFoundError(f"Backup {backup_number} not found")

    def load_range(self, start_backup: int, end_backup: int) -> List[Any]:
        """íŠ¹ì • ë²”ìœ„ì˜ ë°±ì—… íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        all_data = []
        for backup_num in range(start_backup, end_backup + 1):
            try:
                data = self.load_specific_backup(backup_num)
                all_data.extend(data)
            except FileNotFoundError:
                continue
        return all_data

    def __len__(self) -> int:
        """Return the total number of items across all backups and current list."""
        total_items = len(self.product_lists)
        
        for backup_file in os.listdir(self.backup_dir):
            if backup_file.startswith('product_lists_backup_') and backup_file.endswith('.pkl'):
                with open(os.path.join(self.backup_dir, backup_file), 'rb') as f:
                    total_items += len(pickle.load(f))
        
        return total_items

    def __getitem__(self, index):
        """ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ì¸ë±ì‹±ì„ ì§€ì›í•©ë‹ˆë‹¤."""
        if isinstance(index, slice):
            # ìŠ¬ë¼ì´ì‹±ì˜ ê²½ìš°
            return self.get_all_data()[index]
        
        # ë‹¨ì¼ ì¸ë±ìŠ¤ì˜ ê²½ìš°
        if index < 0:  # ìŒìˆ˜ ì¸ë±ìŠ¤ ì²˜ë¦¬
            index = len(self) + index
            
        # í˜„ì¬ ë©”ëª¨ë¦¬ì˜ ë¦¬ìŠ¤íŠ¸ í™•ì¸
        if index < len(self.product_lists):
            return self.product_lists[index]
            
        # ë°±ì—… íŒŒì¼ë“¤ì„ ìˆœíšŒí•˜ë©° í•´ë‹¹ ì¸ë±ìŠ¤ ì°¾ê¸°
        current_pos = len(self.product_lists)
        backup_files = sorted([
            f for f in os.listdir(self.backup_dir)
            if f.startswith('product_lists_backup_') and f.endswith('.pkl')
        ])
        
        for backup_file in backup_files:
            with open(os.path.join(self.backup_dir, backup_file), 'rb') as f:
                backup_data = pickle.load(f)
                backup_len = len(backup_data)
                if current_pos + backup_len > index:
                    # ì´ ë°±ì—… íŒŒì¼ì— í•´ë‹¹ ì¸ë±ìŠ¤ê°€ ìˆìŒ
                    return backup_data[index - current_pos]
                current_pos += backup_len
                
        raise IndexError("list index out of range")

    def __del__(self):
        """Ensure final backup on object destruction."""
        if self.product_lists:
            self._dump_to_pickle()
            
    def get_total_backup_size(self) -> float:
        """ëª¨ë“  ë°±ì—… íŒŒì¼ì˜ ì´ í¬ê¸°ë¥¼ GB ë‹¨ìœ„ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        total_size = 0
        for backup_file in os.listdir(self.backup_dir):
            if backup_file.startswith('product_lists_backup_') and backup_file.endswith('.pkl'):
                file_path = os.path.join(self.backup_dir, backup_file)
                total_size += os.path.getsize(file_path)
        return total_size / (1024 * 1024 * 1024)  # Convert to GB


    def merge_and_save(self, output_path: str, chunk_size_mb: int = 1000) -> bool:
        """
        ëª¨ë“  ë°±ì—… ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            chunk_size_mb: ì²­í¬ ë‹¨ìœ„ í¬ê¸° (MB)
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í˜„ì¬ ë©”ëª¨ë¦¬ì— ìˆëŠ” ë°ì´í„°ë„ ë°±ì—…
            if self.product_lists:
                self._dump_to_pickle()
            
            # ì´ í¬ê¸° í™•ì¸
            total_size_gb = self.get_total_backup_size()
            if total_size_gb > self.max_total_size / (1024 * 1024 * 1024):
                print(f"ê²½ê³ : ì´ í¬ê¸°({total_size_gb:.2f}GB)ê°€ ì œí•œ({self.max_total_size / (1024 * 1024 * 1024)}GB)ì„ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                return False
                
            # ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
            all_data = []
            backup_files = sorted([
                f for f in os.listdir(self.backup_dir)
                if f.startswith('product_lists_backup_') and f.endswith('.pkl')
            ])
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê³  ì“°ê¸°
            chunk_size = chunk_size_mb * 1024 * 1024  # MBë¥¼ bytesë¡œ ë³€í™˜
            current_chunk = []
            current_chunk_size = 0
            
            # ì„ì‹œ íŒŒì¼ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            temp_files = []
            
            for backup_file in backup_files:
                with open(os.path.join(self.backup_dir, backup_file), 'rb') as f:
                    data = pickle.load(f)
                    for item in data:
                        current_chunk.append(item)
                        # ëŒ€ëµì ì¸ í¬ê¸° ì¶”ì •
                        current_chunk_size += sys.getsizeof(str(item))
                        
                        if current_chunk_size >= chunk_size:
                            # ì„ì‹œ íŒŒì¼ì— ì²­í¬ ì €ì¥
                            temp_file = f"{output_path}.temp{len(temp_files)}"
                            with open(temp_file, 'wb') as tf:
                                pickle.dump(current_chunk, tf)
                            temp_files.append(temp_file)
                            current_chunk = []
                            current_chunk_size = 0
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if current_chunk:
                temp_file = f"{output_path}.temp{len(temp_files)}"
                with open(temp_file, 'wb') as tf:
                    pickle.dump(current_chunk, tf)
                temp_files.append(temp_file)
            
            # ëª¨ë“  ì„ì‹œ íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©
            all_data = []
            for temp_file in temp_files:
                with open(temp_file, 'rb') as f:
                    all_data.extend(pickle.load(f))
                os.remove(temp_file)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            
            # ìµœì¢… íŒŒì¼ ì €ì¥
            with open(output_path, 'wb') as f:
                pickle.dump(all_data, f)
            try:
                for backup_file in backup_files:
                    os.remove(os.path.join(self.backup_dir, backup_file))
                print(f"âœ… ë°±ì—…ëœ {len(backup_files)}ê°œ íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âš ï¸ ë°±ì—… íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return True
            
        except Exception as e:
            print(f"ë³‘í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            return False



class SplitTuple(tuple):
    def split(self, sep=None):
        # íŠœí”Œì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì—°ê²°
        joined = ''.join(str(x) for x in self)
        # ë¬¸ìì—´ì„ splití•˜ê³  ê²°ê³¼ë¥¼ ë‹¤ì‹œ SplitTupleë¡œ ë³€í™˜
        return SplitTuple(joined.split(sep))
    
    
def extract_set_from_string(raw_string, keyword_set):
    """
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ `raw_string`ì—ì„œ `keyword_set`ì— í¬í•¨ëœ ëª¨ë“  ë‹¨ì–´ë¥¼ ë¹ ì§ì—†ì´ íƒìƒ‰í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.

    :param raw_string: ê³µë°± ì—†ëŠ” ì…ë ¥ ë¬¸ìì—´
    :param keyword_set: ì°¾ê³ ì í•˜ëŠ” í‚¤ì›Œë“œ ì§‘í•© (set)
    :return: ë§¤ì¹­ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  í‚¤ì›Œë“œë¥¼ ë¹ ì§ì—†ì´ íƒìƒ‰)
    """
    matches = []
    length = len(raw_string)
    
    while raw_string:  # âœ… ë¬¸ìì—´ì´ ë‚¨ì•„ìˆëŠ” ë™ì•ˆ ë°˜ë³µ
        match_found = False
        for size in range(length, 0, -1):  # âœ… ê¸´ í‚¤ì›Œë“œë¶€í„° íƒìƒ‰
            substring = raw_string[:size]  # âœ… ë¬¸ìì—´ì˜ ì²˜ìŒë¶€í„° `size`ë§Œí¼ ê°€ì ¸ì˜¤ê¸°
            if substring in keyword_set:
                matches.append(substring)
                raw_string = raw_string[size:]  # âœ… ë§¤ì¹­ëœ ë¶€ë¶„ ì œê±° í›„ ë‹¤ì‹œ ì²˜ìŒë¶€í„° íƒìƒ‰
                match_found = True
                break  # âœ… ê°€ì¥ ê¸´ ë§¤ì¹­ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ë‹¤ì‹œ ì²˜ìŒë¶€í„° íƒìƒ‰
        if not match_found:
            raw_string = raw_string[1:]  # âœ… ë§¤ì¹­ì´ ì—†ìœ¼ë©´ í•œ ê¸€ìì”© ì´ë™
    
    return matches

def extract_patterns_from_string(raw_string, patterns):
    """
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ íŠ¹ì • ì •ê·œì‹ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ ì¶”ì¶œ.

    :param raw_string: ë„ì–´ì“°ê¸° ì—†ëŠ” ì…ë ¥ ë¬¸ìì—´
    :param patterns: ì°¾ê³ ì í•˜ëŠ” ì •ê·œì‹ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
    :return: íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    """
    matches = []
    index = 0
    length = len(raw_string)
    
    matches = []
    index = 0
    length = len(raw_string)

    while index < length:
        for size in range(length - index, 0, -1):  # ê¸´ í‚¤ì›Œë“œë¶€í„° íƒìƒ‰
            substring = raw_string[index:index+size]
            if any(re.search(pattern, substring) for pattern in patterns):  # âœ… ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ
                matches.append(substring)
                index += size - 1  # âœ… í‚¤ì›Œë“œ ê¸¸ì´ë§Œí¼ ì´ë™
                break
        index += 1  # ë‹¤ìŒ ë¬¸ìë¡œ ì´ë™

    return matches  # âœ… ì°¾ì€ í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜

    #while index < length:
    #    for pattern in patterns:
    #        for size in range(length - index, 0, -1):  # ê¸´ íŒ¨í„´ë¶€í„° ê²€ì‚¬
    #            substring = raw_string[index:index+size]
    #            if re.match(pattern, substring):  # âœ… ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    #                matches.append(substring)
    #                index += size - 1  # âœ… íŒ¨í„´ ê¸¸ì´ë§Œí¼ ì´ë™
    #                break
    #    index += 1  # ë‹¤ìŒ ë¬¸ìë¡œ ì´ë™
    #
    #return matches  # âœ… ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


def extract_year_from_string(raw_string):
    """í•œ ê¸€ìì”© ì´ë™í•˜ë©´ì„œ ì¶œì‹œ ì—°ë„ë¥¼ ê°ì§€"""
    index = 0
    length = len(raw_string)
    detected_years = []

    while index < length - 3:  # ìµœì†Œ 4ìë¦¬ ì—°ë„ ê²€ì‚¬ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        substring = raw_string[index:index+4]
        if re.match(r"^20[2-3][0-9]$", substring):  # âœ… 2020~2029 ë²”ìœ„ ì—°ë„ ê°ì§€
            detected_years.append(substring)
            index += 3  # âœ… ì—°ë„ëŠ” 4ìë¦¬ì´ë¯€ë¡œ, ë‹¤ìŒ íƒìƒ‰ ì¸ë±ìŠ¤ ì¡°ì •
        index += 1
#any(re.match(pattern, substring) for pattern in patterns)
    return list(dict.fromkeys(detected_years))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜





def normalize_product_name(product_name):
    """ë„ì–´ì“°ê¸° ì—†ì´ ì…ë ¥ëœ ë¬¸ìì—´ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ ì •ë¦¬"""
    product_name = convert_none_to_str(product_name)
    
    # âœ… ì‰¼í‘œ ë° `+` ì œê±°
    product_name = product_name.replace(",", " ").replace("+", " ")
    
    # âœ… ê³µë°± ì œê±° í›„ ë¶„ì„ (ë„ì–´ì“°ê¸° ì—†ì´ ë¶™ì–´ ìˆëŠ” ê²½ìš° ëŒ€ë¹„)
    product_name = re.sub(r"\s+", "", product_name)
    
    
    return product_name



def match_score(candidate, reference):
    """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (0~1 ë²”ìœ„)"""
    return SequenceMatcher(None, candidate, reference).ratio()

def extract_keywords_from_string(raw_string, keywords):
    """
    ë„ì–´ì“°ê¸° ì—†ì´ ì…ë ¥ëœ ë¬¸ìì—´ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ ì •ë¦¬.
    - í‚¤ì›Œë“œ ì§‘í•©ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ íƒìƒ‰.
    """
    # âœ… í‚¤ì›Œë“œë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ `set`ì— ì €ì¥
    keyword_set = set()
    for key in keywords:
        keyword_set.update(key.split())  # âœ… ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í‚¤ë¥¼ ë‚˜ëˆ„ì–´ `set`ì— ì €ì¥

    matches = []
    index = 0
    length = len(raw_string)

    while index < length:
        for size in range(length - index, 0, -1):  # ê¸´ í‚¤ì›Œë“œë¶€í„° íƒìƒ‰
            substring = raw_string[index:index+size]
            if substring in keyword_set:  # âœ… ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ
                matches.append(substring)
                index += size - 1  # âœ… í‚¤ì›Œë“œ ê¸¸ì´ë§Œí¼ ì´ë™
                break
        index += 1  # ë‹¤ìŒ ë¬¸ìë¡œ ì´ë™

    return " ".join(matches)  # âœ… ì°¾ì€ í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜

class hashableSlitter(tuple):
    def split(self, sep=None):
        # íŠœí”Œì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì—°ê²°
        joined = ''.join(str(x) for x in self)
        # ë¬¸ìì—´ì„ splití•˜ê³  ê²°ê³¼ë¥¼ ë‹¤ì‹œ SplitTupleë¡œ ë³€í™˜
        return hashableSlitter(joined.split(sep))

def convert_none_to_str(item):
    """Noneì´ë©´ 'None' ë¬¸ìì—´ë¡œ ë³€í™˜, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    return "None" if item is None else item
class ProductDatabasePickleFixed:
    def __init__(self, pickle_filename="product_database.pkl", csv_filename="product_database.csv"):
        self._save_lock = threading.Lock()
        self.pickle_filename = pickle_filename  # âœ… Pickle ì €ì¥ íŒŒì¼
        self.csv_filename = csv_filename  # âœ… ë°°í¬ìš© CSV ì €ì¥ íŒŒì¼
        self.current_id = 1  # ID ì´ˆê¸°ê°’
        self.products = {}  # âœ… ì œí’ˆëª… -> ID ë§¤í•‘
        self.blacklist = set()  # âœ… ë¸”ë™ë¦¬ìŠ¤íŠ¸
        self.product_keywords = set()  # âœ… ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        self.company_tags = None
        self.load_company_tags()
        self.crawl_index = 1  # âœ… í¬ë¡¤ë§ ì¸ë±ìŠ¤ ì´ˆê¸°ê°’
        self.raw_data = {}  # âœ… ID -> ì›ë³¸ ì´ë¦„ ë°ì´í„° ë§¤í•‘
        self.regex_release_year = None
        self.regex_model_number = None
        self.regex_patterns = None
        self.load_regex_patterns()
        # âœ… Pickle íŒŒì¼ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±)
        self.load_or_create_pickle()
         
        
    
    def load_regex_patterns(self, file_path="regex_patterns.txt", default_patterns=None):
        """
        txt íŒŒì¼ì—ì„œ ì •ê·œì‹ íŒ¨í„´ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        ê° ì¤„ì€ "í‚¤:íŒ¨í„´" í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        íŒŒì¼ì´ ì—†ìœ¼ë©´ default_patterns(ì—†ìœ¼ë©´ ë‚´ë¶€ ê¸°ë³¸ê°’)ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ë¡œë“œí•œ íŒ¨í„´ì€ re.compile í›„ self.regex_patternsì— ì €ì¥í•˜ê³ ,
        'release_year'ì™€ 'model_number'ëŠ” ê°ê° self.regex_release_year, self.regex_model_numberì— í• ë‹¹í•©ë‹ˆë‹¤.
        """
        if default_patterns is None:
            default_patterns = {
                "release_year": r"^20[2-3][0-9]$",
                "model_number": r"^(S\d{1,2}|M\d{1,2}|\d+ì„¸ëŒ€|\d{2,4}GB|\d{1,3}(?:\.\d+)?GHz|\d{1,2}A|í´ë“œ\d{1}|ì•„ì´í°\d{1,2}|\d{1}TB)$"
            }
        if not os.path.exists(file_path):
            with open(file_path, "w", encoding="utf-8") as f:
                for key, pattern in default_patterns.items():
                    f.write(f"{key}:{pattern}\n")
            patterns = default_patterns
        else:
            patterns = {}
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and ":" in line:
                        key, pattern = line.split(":", 1)
                        patterns[key.strip()] = pattern.strip()
        self.regex_patterns = {}
        for key, pattern in patterns.items():
            try:
                self.regex_patterns[key] = re.compile(pattern)
            except re.error as e:
                print(f"Error compiling regex for '{key}': {pattern}. Error: {e}")
        # íŠ¹ì • íŒ¨í„´ì„ ë³„ë„ ë³€ìˆ˜ë¡œ í• ë‹¹
        self.regex_release_year = self.regex_patterns.get("release_year", re.compile(r"^20[2-3][0-9]$"))
        self.regex_model_number = self.regex_patterns.get("model_number", re.compile(
            r"^(S\d{1,2}|M\d{1,2}|\d+ì„¸ëŒ€|\d{2,4}GB|\d{1,3}(?:\.\d+)?GHz|\d{1,2}A|í´ë“œ\d{1}|ì•„ì´í°\d{1,2}|\d{1}TB|íŒ¨ë“œ\d{1,2}|\d{1,2}\.\d{1})$"
        ))

    def load_company_tags(self, file_path="company_tags.txt", default_tags=None):
        """
        txt íŒŒì¼ì—ì„œ íšŒì‚¬ íƒœê·¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        ê° ì¤„ì€ "í‚¤:ê°’" í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        íŒŒì¼ì´ ì—†ìœ¼ë©´ default_tags(ì—†ìœ¼ë©´ ë‚´ë¶€ ê¸°ë³¸ê°’)ë¥¼ íŒŒì¼ë¡œ ì €ì¥ í›„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ë¡œë“œí•œ íƒœê·¸ëŠ” self.company_tagsì— í• ë‹¹í•©ë‹ˆë‹¤.
        """
        if default_tags is None:
            default_tags = {
                "ì‚¼ì„±": "ì‚¼ì„±",
                "ì• í”Œ": "APPLE",
                "ì—˜ì§€": "LG",
                "LG": "LG",
                "APPLE": "APPLE",
                "Samsung": "ì‚¼ì„±"
            }
        with self._save_lock:
            if not os.path.exists(file_path):
                with open(file_path, "w", encoding="utf-8") as f:
                    for key, value in default_tags.items():
                        f.write(f"{key}:{value}\n")
                tags = default_tags
            else:
                tags = {}
                with open(file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line and ":" in line:
                            key, value = line.split(":", 1)
                            tags[key.strip()] = value.strip()
        self.company_tags = tags
    
        
        
    def export_raw_data_to_csv(self, file_path):
        """ê²½ë¡œ(file_path)ë¥¼ ì…ë ¥ë°›ì•„ self.raw_dataë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë©”ì„œë“œ.
        
        self.raw_dataëŠ” {product_id: [ { "index":..., "original_name":..., "name":... }, ... ], ... } í˜•íƒœì…ë‹ˆë‹¤.
        ê° í–‰ì—ëŠ” product_id, index, original_name, name í•­ëª©ì´ í¬í•¨ë©ë‹ˆë‹¤.
        """
        import csv
        with self._save_lock:
            with open(file_path, "w", newline="", encoding="utf-8") as csvfile:
                fieldnames = ["product_id", "index", "original_name", "name"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for product_id, records in self.raw_data.items():
                    for record in records:
                        # ê° recordëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. ì—¬ê¸°ì— product_idë¥¼ ì¶”ê°€í•˜ì—¬ í•œ í–‰(row)ë¡œ ë§Œë“­ë‹ˆë‹¤.
                        row = {
                            "product_id": product_id,
                            "index": record.get("index", ""),
                            "original_name": record.get("original_name", ""),
                            "name": record.get("name", "")
                        }
                        writer.writerow(row)
        print(f"âœ… CSV íŒŒì¼ `{file_path}` ì €ì¥ ì™„ë£Œ!")
    def load_or_create_pickle(self):
        hardcoded_blacklist = {
            "Cellular", "í•´ì™¸êµ¬ë§¤", "ì¤‘ê³ ", "Cíƒ€ì…", "5G", "LTE", "í‚¤ë³´ë“œ", "ë¶ì»¤ë²„", "ì¼€ì´ìŠ¤",
            "ë§¤ì§í‚¤ë³´ë“œ", "í´ë¦¬ì˜¤ì¼€ì´ìŠ¤", "ì• í”ŒíœìŠ¬", "í´ë¦¬ì˜¤í‚¤ë³´ë“œ", "í´ë¦¬ì˜¤ì¼€ì´ìŠ¤","í´ë¦¬ì˜¤í‚¤ë³´ë“œ","ìê¸‰ì œ"
        }
            # âœ… ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ê¸°ë³¸ê°’
        hardcoded_product_keywords = {
        "iPad", "ê°¤ëŸ­ì‹œíƒ­", "ê°¤ëŸ­ì‹œZ","Surface", "GíŒ¨ë“œ", "Yoga", "Xperia", "MatePad","ê°¤ëŸ­ì‹œ","ìš¸íŠ¸ë¼","í”Œë¦½","í´ë“œ"
        "ëƒ‰ì¥ê³ ", "ì„¸íƒê¸°", "TV", "ì˜¤ë¸", "ì „ìë ˆì¸ì§€", "ì—ì–´ì»¨", "ê³µê¸°ì²­ì •ê¸°", "iPhone","ì•„ì´í°","ë§¥ë¶","ë§¥ë¯¸ë‹ˆ","ë§¥ë¶ì—ì–´","ë§¥ë¶í”„ë¡œ",
        "ì²­ì†Œê¸°", "ìŠ¤í”¼ì»¤", "ëª¨ë‹ˆí„°", "M1", "M2", "M3", "M4", "M5", "mini", "Air","í”ŒëŸ¬ìŠ¤","ìš”ê°€","ë ˆë…¸ë²„","Pad","AI","ìƒ¤ì˜¤ë¯¸","ë¯¸","íŒ¨ë“œ","í”„ë¡œ"
        }
        hardcoded_product = {
            "APPLE 2021 iPad Pro 11 3ì„¸ëŒ€": 1,
            "APPLE 2021 iPad Pro 12.9 5ì„¸ëŒ€": 2,
            "APPLE 2022 iPad Air 5ì„¸ëŒ€": 3,
            "APPLE 2022 iPad Pro 11 4ì„¸ëŒ€": 4,
            "APPLE 2022 iPad Pro 12.9 6ì„¸ëŒ€": 5,
            "APPLE 2024 iPad Air 13 M2": 6,
            "APPLE 2024 iPad Air 13 M2 í”„ë¡œ": 7,
            "APPLE 2024 iPad mini A17 Pro 7ì„¸ëŒ€": 8,
            "APPLE 2024 iPad Pro 11 M4": 9,
            "APPLE 2024 iPad Pro 11 M4 í”„ë¡œ": 10,
            "APPLE 2024 iPad Pro 13 M4": 11,
            "APPLE 2024 iPad Pro 13 M4 í”„ë¡œ": 12,
            "ë ˆë…¸ë²„ ìš”ê°€ Pad Pro AI": 13,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S10 ìš¸íŠ¸ë¼": 14,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S10 í”ŒëŸ¬ìŠ¤": 15,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S8": 16,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S8 ìš¸íŠ¸ë¼": 17,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S8 í”ŒëŸ¬ìŠ¤": 18,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S9": 19,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S9 ìš¸íŠ¸ë¼": 20,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S9 í”ŒëŸ¬ìŠ¤": 21,
            "ìƒ¤ì˜¤ë¯¸ ë¯¸ íŒ¨ë“œ7": 22,
            "ìƒ¤ì˜¤ë¯¸ ë¯¸ íŒ¨ë“œ7 í”„ë¡œ": 23
                }

        """Pickle íŒŒì¼ì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±"""

        if not os.path.exists(self.pickle_filename):
            self.blacklist = hardcoded_blacklist  # âœ… ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í•˜ë“œì½”ë”© ê°’ ì¶”ê°€
            print(f"âš ï¸ Pickle íŒŒì¼ `{self.pickle_filename}`ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            self.product_keywords = hardcoded_product_keywords
            self.products = hardcoded_product
            self.current_id = 24 
            self.save_to_pickle()  # âœ… ê¸°ë³¸ê°’ìœ¼ë¡œ íŒŒì¼ ìƒì„±
        else:
            with open(self.pickle_filename, "rb") as f:
                data = pickle.load(f)
                self.products = data.get("products", {})
                self.blacklist = data.get("blacklist", set())
                self.product_keywords = data.get("product_keywords", set())
                if isinstance(self.products, set):
                    print("âŒ ì˜¤ë¥˜: productsê°€ `set`ìœ¼ë¡œ ì €ì¥ë¨! ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                    self.products = hardcoded_product  # âœ… ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
                    self.current_id = 23
                self.current_id = max(self.products.values(), default=1) + 1  # âœ… ID ìë™ ì¦ê°€

    def save_to_pickle(self):
        """í˜„ì¬ ë°ì´í„°ë¥¼ Pickle íŒŒì¼ì— ì €ì¥"""
        with self._save_lock:
            with open(self.pickle_filename, "wb") as f:
                pickle.dump({
                    "products": self.products,
                    "blacklist": self.blacklist,
                    "product_keywords": self.product_keywords
                }, f)
        print(f"âœ… Pickle íŒŒì¼ `{self.pickle_filename}` ì €ì¥ ì™„ë£Œ!")

    def detect_company(self, product_name):
        """ì œí’ˆëª…ì—ì„œ íšŒì‚¬ íƒœê·¸ë¥¼ ê°ì§€í•˜ì—¬ ìë™ ë¶„ë¥˜"""
        for tag, company in self.company_tags.items():
            if tag in product_name:
                return company
        return "ê¸°íƒ€"

    def export_original_name(self):
        out=[]
        for product_id, records in self.raw_data.items():
            for record in records:
                # ê° recordëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. ì—¬ê¸°ì— product_idë¥¼ ì¶”ê°€í•˜ì—¬ í•œ í–‰(row)ë¡œ ë§Œë“­ë‹ˆë‹¤.
                row = {
                    "product_id": product_id,
                    "index": record.get("index", ""),
                    "original_name": record.get("original_name", ""),
                    "name": record.get("name", "")
                }
                out.append(row)
        return out
    def filter_and_standardize(self, product_name):
        """ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° í›„, ê°€ì¥ ì í•©í•œ ë‹¨ì–´ ìˆœì„œë¡œ ì •ë¦¬í•˜ì—¬ ëª¨ë¸ëª…ì„ ìƒì„±"""
        original_name = convert_none_to_str(product_name)  # âœ… ì›ë³¸ ì œí’ˆëª… ì €ì¥
        product_name = normalize_product_name(original_name)  # âœ… ê³µë°± ì œê±° í›„ ì •ë¦¬
        for word in self.blacklist:
            product_name = product_name.replace(word, " ")
            product_name = re.sub(r"\s+", " ", product_name).strip()
        extracted_name = extract_keywords_from_string(product_name, self.products.keys())
        words = extracted_name.split()


        # âœ… ì¶œì‹œ ì—°ë„ ê°ì§€ (ì˜ˆ: 2022, 2023, 2024 ë“±)
        #year_match = extract_patterns_from_string(product_name, [r"^20[2-3][0-9]$"])
        year_match=re.match(self.regex_release_year,word)
        #[word for word in words if re.match(r"^20[2-3][0-9]$", word)]
        release_year = year_match[0] if year_match else ""

        # âœ… ëª¨ë¸ ë„˜ë²„ ê°ì§€ (ì˜ˆ: "11", "12.9", "S8", "M2", "M4", "6ì„¸ëŒ€" ë“±)
        #model_numbers = extract_patterns_from_string(product_name, [r"^(S\d{1,2}|M\d{1,2}|\d+ì„¸ëŒ€|\d{2,4}GB|\d{1,3}(\.\d+)?GHz|\d{1,2}A|\Ad{1,2}|í´ë“œ\d{1}|ì•„ì´í°\d{1,2}|\d{1}TB)$"])
        model_numbers=extract_patterns_from_string(product_name,[self.regex_model_number])
        keyword_member = extract_set_from_string(product_name, self.product_keywords)
        #[word for word in words if re.match(r"^(\d+(\.\d+)?|S\d+|M\d+|\d+ì„¸ëŒ€+Air)$", word)]
        # âœ… í•˜ë“œì½”ë”©ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ì™€ ë¹„êµí•˜ì—¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì°¾ê¸°
        common_words_set = set()
        for model in self.products:
            common_words = set(words) & set(model.split())  # âœ… ì œí’ˆëª…ê³¼ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì¶”ì¶œ
            if common_words:
                common_words_set = common_words
                break  # âœ… í•œ ë²ˆì´ë¼ë„ ê²¹ì¹˜ë©´ ë°”ë¡œ ì‚¬ìš©
        # âœ… ë“±ë¡ë˜ì§€ ì•Šì€ ë¬¸êµ¬ë„ ì¶”ê°€ë¡œ ë°˜ì˜
        # âœ… ì œì¡°ì‚¬ ê°ì§€ (ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€)
        detected_product = self.detect_company(original_name)
        # âœ… ê°€ì¥ ì í•©í•œ ì œí’ˆëª… ì°¾ê¸°
        total_word=[]
        total_word.append(detected_product)
        for i in common_words:
            total_word.append(i)
        for i in model_numbers:
            total_word.append(i)
        total_word.append(release_year)
        total_word.extend(keyword_member)
        unregistered_word=product_name
        for word in total_word:
            unregistered_word = unregistered_word.replace(word, " ")    
        unregistered_words =[word for word in unregistered_word.split() if len(word) >21] 
        total_word.extend(unregistered_words)
        total_word = ["ì‚¼ì„±ì „ì" if word == "ì‚¼ì„±" else word for word in total_word]
        common_words_set = set(total_word)
        best_match = self.match_best_product(common_words_set,original_name)
        # âœ… ìµœì¢… ëª¨ë¸ëª… ìƒì„± (ì œì¡°ì‚¬ + ì •ë ¬ëœ ëª¨ë¸ëª… + ëª¨ë¸ ë„˜ë²„ë§ + ì¶œì‹œ ì—°ë„)
        print(f" before join âœ… ëœ ì œí’ˆëª…: {best_match}, ì›ë³¸: {original_name}, Process Number: {self.current_id}")
        # âœ… ê¸°ì¡´ ì œí’ˆëª…ì´ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ì¡°í•© ìƒì„±
        standardized_name = ' '.join(best_match).strip() if best_match else f"{' '.join(common_words_set)}".strip()

        print(f"âœ… ëœ ì œí’ˆëª…: {standardized_name}, ì›ë³¸: {original_name}, Process Number: {self.current_id}")
        return original_name,standardized_name   # âœ… ì›ë³¸, ì •í™”ëœ ì œí’ˆëª… í•¨ê»˜ ë°˜í™˜

    def add_product(self, product_name):
        """ìƒˆë¡œìš´ ì œí’ˆì„ ì¶”ê°€í•˜ë©´ì„œ ì¸¡ì • ì¸ë±ìŠ¤ + ì›ë³¸ ì œí’ˆ ID + ì˜¤ì—¼ ëª¨ë¸ëª…ì„ í•¨ê»˜ ì €ì¥"""
        original_name, standardized_product = self.filter_and_standardize(product_name)  # âœ… ì›ë³¸ & ëœ ì œí’ˆëª…
        
        # âœ… ê¸°ì¡´ ì œí’ˆì´ë©´ ê¸°ì¡´ ID ìœ ì§€ (í•˜ì§€ë§Œ original_nameë„ ì €ì¥í•´ì•¼ í•¨!)
        if standardized_product in self.products:
            assigned_id = self.products[standardized_product]
        else:
            # âœ… ìƒˆë¡œìš´ ì œí’ˆì´ë©´ ìƒˆë¡œìš´ ID í• ë‹¹
            assigned_id = self.current_id
            self.products[standardized_product] = assigned_id
            self.current_id += 1  # âœ… ìƒˆë¡œìš´ ì œí’ˆì´ ì¶”ê°€ë  ë•Œë§Œ ì¦ê°€!

        # âœ… ì›ë³¸ ë°ì´í„° ì €ì¥ (ê¸°ì¡´ ì œí’ˆì´ë“  ìƒˆë¡œìš´ ì œí’ˆì´ë“  ì €ì¥!)
        if assigned_id not in self.raw_data:
            self.raw_data[assigned_id] = []  # âœ… IDë³„ë¡œ ì—¬ëŸ¬ ì›ë³¸ëª…ì„ ì €ì¥ ê°€ëŠ¥í•˜ë„ë¡ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€ê²½

        self.raw_data[assigned_id].append({
            "index": self.crawl_index,
            "original_name": original_name,
            "name": standardized_product
        })
        self.crawl_index += 1
        # âœ… Pickleì— ì €ì¥
        self.save_to_pickle()
    def match_best_product(self, sorted_product,original_order):
        """
        ì›ë³¸ ì œí’ˆëª…(`original_order`)ì˜ ë‹¨ì–´ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ `sorted_product`ì˜ ë‹¨ì–´ë“¤ì„ ì¬ë°°ì—´.
        :param original_order: ì›ë³¸ ì œí’ˆëª… ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        :param sorted_product: ì •ë ¬í•´ì•¼ í•  ë‹¨ì–´ ì§‘í•© (set)
        :return: ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        """
        original_order = original_order.replace(" ", "")  # âœ… ê³µë°± ì œê±° í›„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°
        matched_words = []  # âœ… ë§¤ì¹­ëœ ë‹¨ì–´ ì €ì¥
        remaining_string = original_order  # âœ… íƒìƒ‰í•  ë¬¸ìì—´ ì´ˆê¸°í™”
        match_found = False
        while remaining_string:  # âœ… ë¬¸ìì—´ì´ ë‚¨ì•„ìˆëŠ” ë™ì•ˆ ë°˜ë³µ
            for word in sorted_product:
                if not word:
                    continue
                if (word==remaining_string[0:len(word)]) and (word not in matched_words):  # âœ… ë‹¨ì–´ê°€ ì¡´ì¬í•˜ë©´ ë°˜ë³µì ìœ¼ë¡œ ì œê±°
                    matched_words.append(word)  # âœ… ë§¤ì¹­ëœ ë‹¨ì–´ ì €ì¥
                    remaining_string = remaining_string.replace(word, "", 1)
                    match_found = True
            if not match_found:
                remaining_string = remaining_string[1:]
            else:
                match_found = False        
        return matched_words
    def export_to_csv(self):
        """í˜„ì¬ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (ë°°í¬ìš©)"""
        import pandas as pd
        df = pd.DataFrame(self.products.items(), columns=["Standardized_Product", "ID"])
        with self._save_lock:
            df.to_csv(self.csv_filename, index=False)
        print(f"âœ… CSV íŒŒì¼ `{self.csv_filename}` ì €ì¥ ì™„ë£Œ!")
    def add_to_blacklist(self, item):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ë‹¨ì–´ ì¶”ê°€"""
        self.blacklist.add(item)
        self.save_to_pickle()

    def remove_from_blacklist(self, item):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì œê±°"""
        if item in self.blacklist:
            self.blacklist.remove(item)
            self.save_to_pickle()

    def read_blacklist(self):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        return list(self.blacklist)

    def add_to_product_list(self, item):
        """ì œí’ˆêµ° ë¦¬ìŠ¤íŠ¸ì— ë‹¨ì–´ ì¶”ê°€"""
        self.product_keywords.add(item)
        self.save_to_pickle()

    def remove_from_product_list(self, item):
        """ì œí’ˆêµ° ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì œê±°"""
        if item in self.product_keywords:
            self.product_keywords.remove(item)
            self.save_to_pickle()

    def read_product_list(self):
        """ì œí’ˆêµ° ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        return list(self.product_keywords)

class TapName:
    def __init__(self, driver):
        self.opinion = "productOpinion"
        self.companyReview = "companyReview"
        self.mcReview = "mcReview"
        self.front = "danawa-prodBlog-"
        self.mid = "-button-tab-"
        self.driver = driver
        self.review=[]
        self.average_star=None
        #review Parsing parm name
        self.second_selector = 'div.common_paginate'
        self.third_selector = 'div.nums_area'
    def getfirst(self):
        html = self.driver.page_source
        try:
            soup = BeautifulSoup(html, 'html.parser')
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
            tab.click()
        except:
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.companyReview+self.mid+self.opinion)))
            tab.click()
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
            tab.click()
            
        result=self.extract_reviews_general(soup)
        self.get_avg_star()
        return result
    def get_avg_star(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        selector="#danawa-prodBlog-productOpinion-list-self > div.mall_review > div.area_left > div.grade_area > div.grd_stats > div.point_num > strong"
        elems = soup.select(selector)
        if elems:
            self.average_star = elems[0].get_text(strip=True)
        else:
            self.average_star = "no review"
        self.get_second()
    
    def get_second(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        result=self.extract_reviews_general(soup)
        return result
    def oneshot(self):
        out=[]
        out.append(self.getfirst())
        out.extend(self.get_second())
        return out
    def clean(self):
        self.driver.quit()
        return self.review
    def oneshot_iter(self):
        out=[]
        try:
            out.extend(self.oneshot())
        except:
            try:
                flag=True
                n=2
                while flag:
                    flag=self.click_opinion_page(n)
                    if flag:
                        out.extend(self.get_second())
                        n=n+1                   
            except:
                if out:
                    return out
                else:
                    return out.append(["no review"])
        try:
            flag=True
            n=2
            while flag:
                flag=self.click_opinion_page(n)
                if flag:
                    out.extend(self.get_second())
                    n=n+1                   
        except:
            if out:
                return out
            else:
                return out.append(["no review"])
        return out    

    def extract_reviews_general(self, soup, similarity_threshold=0.7):
        try:
            reviews = []
            candidate_selectors = [
                'p.danawa-prodBlog-productOpinion-list-self',
                ("div", {"id": "danawa-prodBlog-productOpinion-list-self"}),
                ('div', {"id": "danawa-prodBlog-companyReview-content-list"}),
            ]
            # í›„ë³´ ì…€ë ‰í„°ë“¤ë¡œë¶€í„° ë¦¬ë·° ìš”ì†Œ(filtered_elems) ìˆ˜ì§‘
            for selector in candidate_selectors:
                if isinstance(selector, str):
                    elems = soup.select(selector)
                else:
                    elems = soup.find_all(*selector)
                if elems:
                    filtered_elems = []
                    for elem in elems:
                        parent_div = elem.find_all('li', class_="danawa-prodBlog-companyReview-clazz-more")
                        if parent_div:
                            filtered_elems.extend(parent_div)
                        else:
                            parent_div = elem.find_all(
                                'p',
                                class_="danawa-prodBlog-productOpinion-clazz-content",
                                attrs={"data-seq": "N"}
                            )
                            if parent_div:
                                for i in parent_div:
                                    if isinstance(i, str):
                                        filtered_elems.append(i.replace("\n", "").replace("\t", ""))
                                    else:
                                        filtered_elems.extend(i.text.replace("\n", "").replace("\t", ""))
                                filtered_elems.extend(parent_div)
            # ê° ë¦¬ë·° ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸(ë° ë³„ì ) ì¶”ì¶œ
            for felem in filtered_elems:
                text_elements = felem.find_all('div', class_="atc")
                if self.average_star:
                    star = felem.find_all('span', class_="star_mask")
                    if text_elements:
                        for elem in text_elements:
                            if hasattr(elem, "get_text"):
                                cleaned=elem.get_text()
                            else:
                                cleaned = str(elem)
                                    #if hasattr(elem, "get_text"):
                                    #    cleaned = " ".join(set(elem.get_text(separator=' ', strip=True).split()))
                                    #else:
                                    #    cleaned = " ".join(set(str(elem).split()))
                            if cleaned:
                                # ë³„ì ì´ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶”ê°€
                                if star and len(star) > 0:
                                    reviews.append([cleaned, star[0].get_text(strip=True)])
                                else:
                                    reviews.append(cleaned)

            # ì¤‘ë³µ ì œê±° (Jaccard ìœ ì‚¬ë„ ê¸°ì¤€)
            #unique_reviews = []
            #for review in reviews:
            #    is_duplicate = False
            #    for unique_review in unique_reviews:
            #        if jaccard_similarity(review, unique_review) > similarity_threshold:
            #            is_duplicate = True
            #            break
            #    if not is_duplicate:
            #        unique_reviews.append(review)
            return reviews
        except Exception as e:
            print("error", e)
            return reviews
    def click_opinion_page(self, page):
        try:
            # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (ìˆ˜ì •ë¨: CSS Selectorë¡œ ë³€ê²½)
            first = self.driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-companyReview-content-list")
            if not first:
                return False

                
            # âœ… ë‘ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.second_selector ì‚¬ìš©)
            Spagr = self.driver.find_elements(By.CSS_SELECTOR, self.second_selector)
            if not Spagr:
                return False
            if page%10==1:
                try:
                    page_next_buttons = WebDriverWait(self.driver, 10).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, "a.nav_edge_next.nav_edge_on"))
                    )
                    page_next_buttons.click()
                    return True
                except:
                    return False
             #âœ… ì„¸ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.third_selector ì‚¬ìš©)
            Tpagr = self.driver.find_elements(By.CSS_SELECTOR, self.third_selector)
            if not Tpagr:
                return False
            #if page%10==0:
                #try:
                #    page_last_buttons = WebDriverWait(self.driver, 10).until(
                #            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#danawa-prodBlog-companyReview-content-list > div > div > div > span"))
                #                )
                #    page_last_buttons[0].click()
                #    return True
                #except:
                #    return False
            # âœ… í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (WebDriverWait ì ìš©)
            page_buttons = WebDriverWait(self.driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.page_num[data-pagenumber]"))
            )
            # âœ… íŠ¹ì • í˜ì´ì§€ ë²ˆí˜¸ í´ë¦­ (ë¬¸ìì—´ ë³€í™˜ ì¶”ê°€)
            for button in page_buttons:
                page_number = button.get_attribute("data-pagenumber")
                if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                    self.driver.execute_script("arguments[0].click();", button)
                    return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜    
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
            return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜
        return False  # í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ì„ ê²½ìš° False
 
def review_loop (url,product_list,save_dir_image,trynum=5):
    options = Options()
    options.add_argument('--headless') 
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument("--disable-dev-shm-usage")  # âœ… `/dev/shm` ë¶€ì¡± ë¬¸ì œ í•´ê²°
    options.add_argument("--remote-debugging-port=9222")  # âœ… ë””ë²„ê¹… í™œì„±í™”
    driver = webdriver.Chrome(service = Service(),options=options)
    

    
    # í¬ë¡¤ë§í•  í˜ì´ì§€ URL (ì‹¤ì œ URLë¡œ ë³€ê²½)
    if len(url)<5:
        print("url")
        print ["no review"]
    num=0
    loopcon=True
    while loopcon:
        try:
            driver.get(url)
            loopcon=False
        except:
            print("fail")
            sleep(1)
            num=num+1
            if num>10:
                print ["can't read review page_driver"]
                return False
    product_list["product_link"] = []
    
    num=0
    loopcon=True
    while loopcon:
        try:
            html = driver.page_source
            loopcon=False
        except:
            print("fail")
            sleep(1)
            num=num+1
            if num>20:
                print ["can't read review page_html"]
                return False
    

      # Seleniumì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ HTML
    # ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•´ ì¶©ë¶„í•œ ì‹œê°„ ëŒ€ê¸° (WebDriverWait ì‚¬ìš© ê¶Œì¥)

    num=0
    loopcon=True
    while loopcon:
        try:
            WebDriverWait(driver, 3).until(lambda driver: driver.execute_script("return document.readyState") == "complete")
            loopcon=False
        except:
            print("fail")
            sleep(1)
            num=num+1
            if num>10:
                print ["can't read review page_driver_WebDriverWait1"]
                return False

    # ë‘ ê°œì˜ ì˜ê²¬/ë¦¬ë·° íƒ­ ID ëª©ë¡ (í•„ìš”í•œ ê²½ìš° ë‘ íƒ­ ëª¨ë‘ ì˜ê²¬/ë¦¬ë·° ë²”ì£¼ì— í•´ë‹¹í•œë‹¤ê³  ê°€ì •)
    desired_tab_ids = {
        "danawa-prodBlog-productOpinion-button-tab-productOpinion",
        "danawa-prodBlog-productOpinion-button-tab-companyReview"
    }
    num=0
    loopcon=True
    while loopcon:
        try:
            active_tab_elements = WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.tab_item"))
            )
            loopcon=False
        except:
            print("fail")
            sleep(1)
            num=num+1
            if num>10:
                print ["can't read review page"]
            break

    soup = BeautifulSoup(html, 'html.parser')
    try:
        affiliate_div = soup.select_one("#AffiliateMallListDiv")
        if affiliate_div:
            link_tags = affiliate_div.select("div > div.d_dsc > div.prc_line > a")
            for tag in link_tags:
                product_list["product_link"].append(tag.get("href"))
        openmarket_div = soup.select_one("#OpenMarketMallListDiv")
        if openmarket_div:
            link_tags = openmarket_div.select("div > div.d_dsc > div.prc_line > a")
            for tag in link_tags:
                product_list["product_link"].append(tag.get("href"))
    except:
        print("Error can't find product market link. skip this procedure")
    html=driver.page_source    
    soup = BeautifulSoup(html, 'html.parser')
    ##NEXT IMAGEFUN  save_dir_image
    image_tag = soup.select_one("#baseImage")
    if image_tag:
        image_src = image_tag.get('src')
        download_image(image_src, save_dir_image, product_list['name'])
        product_list['image'] = {
                'src': image_src,
                'saved_path': os.path.join(save_dir_image, product_list['name'])
                        }
    #driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-productOpinion-button-tab-productOpinion")
    for active_tab_element in active_tab_elements:
        classname = active_tab_element.get_attribute("class")
        if classname.endswith("on"):
            if not 'bookmark_cm_opinion_item'==active_tab_element.get_attribute("id"):
                        T=0
                        while T<trynum:
                            try:
                                product_opinion_tab = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "#bookmark_cm_opinion_item > a")))
                                driver.execute_script("arguments[0].click();", product_opinion_tab)
                                break
                            except:
                                T=T+1
                                print("retry")
                        if T==trynum:
                            print("fail")
                            return False
            else:
                break
    # ì›í•˜ëŠ” ì‹œì‘ ë…¸ë“œë¥¼ ì§€ì •í•œ í›„, íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    html = driver.page_source  # Seleniumì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ HTML
    soup = BeautifulSoup(html, 'html.parser')
    try:
        reviews=TapName(driver)
        out=reviews.oneshot_iter()
        driver.quit()
        return out
    except:
        print("fail resource release")
        return reviews.clean()


def jaccard_similarity(text_1, text_2):
    """ ë‘ ê°œì˜ ë¬¸ì¥ ê°„ Jaccard Similarity ê³„ì‚° """
    if isinstance(text_1, list):
        text1 = text_1[0]
    else:
        text1 = text_1
    if isinstance(text_2, list):
        text2 = text_2[0]
    else:
        text2 = text_2
    set1, set2 = set(text1.split()), set(text2.split())
    intersection = len(set1 & set2)  # ê³µí†µ ë‹¨ì–´ ê°œìˆ˜
    union = len(set1 | set2)  # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜
    return intersection / union if union != 0 else 0

def click_page(page,driver,timeout=10):
    try:
        # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (í˜ì´ì§€ ì½˜í…ì¸ ê°€ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#productListArea"))
        )
        if page%10==1:
            try:
                next_page_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "#productListArea div.prod_num_nav a.edge_nav.nav_next"))
                )
                next_page_button.click()
                return True
            except:
                try:
                    driver.execute_script("arguments[0].click();", next_page_button)
                    return True
                except:
                    return False
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.number_wrap"))
        )
        page_buttons = driver.find_elements(By.CSS_SELECTOR, "div.number_wrap a.num")
        for button in page_buttons:
            page_number = button.text.strip()
            if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                driver.execute_script("arguments[0].click();", button)
                WebDriverWait(driver, timeout).until(EC.staleness_of(button))
                return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜
        return False  # âœ… í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ì„ ê²½ìš° False 
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
        return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜



def get_data_from_url_single(url,num,save_dir_image,fail,limiter,reviewfactor,clean_item):
    index_i=0
    options = Options()
    
    options.add_argument('--headless')      
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(service = Service(),options=options)
    product_list=[]
    try:
        # âœ… í˜ì´ì§€ ì´ë™
        driver.get(url)
        # âœ… ì²« ë²ˆì§¸ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")
        # âœ… í˜ì´ì§€ ì´ë™ (num í˜ì´ì§€ í´ë¦­)
        if num != 1:
            count=click_page(num, driver)  
            if not count:
                fail+=1
            WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")
        html = driver.page_source
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'html.parser')
        # ì œí’ˆ ì •ë³´ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì—¬ëŸ¬ ìƒí’ˆì´ ìˆì„ ê²½ìš° ë°˜ë³µë¬¸ ì‚¬ìš©)
        all_product_divs = soup.find_all('div', class_='prod_main_info')
        # ì—¬ëŸ¬ ìƒí’ˆ ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        debug=0
        for prod_info_div in all_product_divs:
            R=None
            with open('output/flag.txt', 'r') as f:
                R=f.readline()
            if (R.strip() != "1"):
                break  
            product_info = {}
            if prod_info_div:
                # ì œí’ˆëª… ì¶”ì¶œ: <p class="prod_name"> ë‚´ë¶€ì˜ <a name="productName"> íƒœê·¸
               
                prod_name_tag = prod_info_div.find('p', class_='prod_name')
                if prod_name_tag:
                    a_tag = prod_name_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = a_tag.get_text(strip=True)
                prod_op_link_tag = prod_info_div.find('p', class_='prod_name')
                if prod_op_link_tag:
                    op_link_tag = prod_op_link_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = op_link_tag.get_text(strip=True)
                        product_info['opinion'] = {} 
                        product_info['opinion']['link'] = op_link_tag.get('href')
                comment_div = prod_info_div.find('div', class_='meta_item mt_comment')
                if comment_div:
                    a_comment = comment_div.find('a')
                    if a_comment:
                        # ì œí’ˆì˜ê²¬ ê´€ë ¨ ì •ë³´: ë§í¬, ì˜ê²¬ ìˆ˜ ë“±
                        product_info['opinion']['sub_link'] = a_comment.get('href')
                        strong_tag = a_comment.find('strong')
                        if strong_tag:
                            product_info['opinion']['count'] = strong_tag.get_text(strip=True)
                # ìŠ¤í™ ì˜ì—­ ì¶”ì¶œ: <div class="spec_list"> ë‚´ë¶€ì˜ ëª¨ë“  ìŠ¤í™ ì •ë³´ ì¶”ì¶œ
                spec_items = []
                spec_div = prod_info_div.find('div', class_='spec_list')
                if spec_div:
                    # <a class="view_dic"> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ì¶”ì¶œ
                    for a in spec_div.find_all('a', class_='view_dic'):
                        if a:
                            text = a.get_text(strip=True)
                            spec_items.append(text)
                ###NEXT IMAGEFUN  save_dir_image
                #image_tag = prod_info_div.find('img')
                #if image_tag:
                #    image_src = image_tag.get('src')
                #    download_image(image_src, save_dir_image, product_info['name'])
                #    product_info['image'] = {
                #            'src': image_src,
                #            'saved_path': os.path.join(save_dir_image, product_info['name'])
                #        }
                price_info=extract_prod_info_list(prod_info_div)
                #download_image(ëª©í‘œì£¼ì†Œ, save_dir_image, ì´ë¦„)
                product_info['price'] = price_info
                # ì¶”ì¶œí•œ ìŠ¤í™ ë°°ì—´ì„ ì œí’ˆ ì •ë³´ì— ì¶”ê°€ (ë°°ì—´ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê±°ë‚˜, join()ìœ¼ë¡œ ë¬¸ìì—´ ê²°í•© ê°€ëŠ¥)
                product_info['specs'] = spec_items
                print (f"debug print : {product_info['name']}")
                clean_item.add_product(product_info['name'])
                product_list.append(product_info)
            if reviewfactor:    
                if 'opinion' in product_list[index_i]:
                    try:
                        product_list[index_i]['opinion']["reviews"] =review_loop(product_list[index_i]['opinion']['link'],product_list[index_i],save_dir_image)
                        index_i+=1
                    except:
                        index_i+=1
                        product_list[index_i]['opinion']["reviews"] = ["no review"]
                        product_list[index_i]['product_link'] = ["no link"]
            debug+=1
            if debug>=limiter:
                break       
                        
        return product_list, fail
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ maim loop: {e}")
        if not product_list:
            product_list = ["get fail"]
        return product_list,fail
    finally:

        driver.quit()  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¸Œë¼ìš°ì €ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
def extract_prod_info_list(soup):
    """
    idê°€ "productInfoDetail_"ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  <li> ìš”ì†Œ ë‚´ë¶€ì—ì„œ,
      - <p> íƒœê·¸ì˜ classë¥¼ í‚¤ë¡œ, ê·¸ ë‚´ìš©ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” í•­ëª©ë“¤ì„ ì¶”ì¶œí•˜ê³ ,
      - ë³„ë„ë¡œ ê°€ê²© ì •ë³´(p.price_sect > a > strong)ë¥¼ "price" í‚¤ì— ì €ì¥í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    ê° <li> ë³„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result_list = []
    
    # idê°€ "productInfoDetail_"ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  <li> ìš”ì†Œ ì°¾ê¸°
    li_elements = soup.select('li[id^="productInfoDetail_"]')
    
    for li in li_elements:
        product_dict = {}
        
        # li ë‚´ë¶€ì˜ ëª¨ë“  <p> íƒœê·¸ ìˆœíšŒ
        p_tags = li.find_all('p')
        for p in p_tags:
            # p íƒœê·¸ì— class ì†ì„±ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
            p_classes = p.get("class")
            if not p_classes:
                continue
            
            # ê°€ê²© ì •ë³´ìš© p íƒœê·¸("price_sect")ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
            if "price_sect" in p_classes:
                continue
            
            # p íƒœê·¸ì˜ class ëª©ë¡ì„ ê³µë°±ìœ¼ë¡œ í•©ì³ keyë¡œ ì‚¬ìš©
            key = " ".join(p_classes)
            # p íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸(ìì‹ íƒœê·¸ í¬í•¨)ë¥¼ valueë¡œ ì‚¬ìš©
            value = p.get_text(separator=" ", strip=True)
            
            product_dict[key] = value
        
        # ê°€ê²© ì •ë³´ ì¶”ì¶œ: li ë‚´ë¶€ì˜ 'p.price_sect > a > strong'
        price_tag = li.select_one("p.price_sect a strong")
        if price_tag:
            product_dict["price"] = price_tag.get_text(strip=True)
        
        result_list.append(product_dict)
    
    return result_list
def download_image(image_url, save_dir, filename=None):
    """
    image_url: ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ URL (ì˜ˆ: '//img.danawa.com/...' í˜•ì‹)
    save_dir: ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ
    filename: ì €ì¥í•  íŒŒì¼ëª… (Noneì´ë©´ URLì—ì„œ ì¶”ì¶œ)
    """
    # URLì´ í”„ë¡œí† ì½œ ì—†ì´ // ë¡œ ì‹œì‘í•˜ë©´ https: ì¶”ê°€
    if image_url.startswith('//'):
        image_url = 'https:' + image_url
    
    # ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(save_dir, exist_ok=True)
    
    # filenameì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œê±°)
    if not filename:
        filename = os.path.basename(image_url.split('?')[0])
    save_path = os.path.join(save_dir, filename)
    
    try:
        response = requests.get(image_url, stream=True)
        response.raise_for_status()  # ìƒíƒœ ì½”ë“œ ì²´í¬
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        print(f"ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {image_url} -> {save_path}")
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url} ì—ëŸ¬: {e}")

def extract_name(manager, fname="output/danawa.csv"):
    out = []
    
    # ProductListManagerì˜ ê²½ìš° ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°
    data = manager.get_all_data() if hasattr(manager, 'get_all_data') else manager
    
    for d in data:
        if not (d[0] == 'get fail'):
            for i in d:
                if isinstance(i, bytes):
                    try:
                        R = ast.literal_eval(i.decode("utf-8"))
                        out.append(R["name"])
                    except Exception as e:
                        print(f"Error processing item: {e}")
                        continue

    out = pd.DataFrame(out)
    out.to_csv(fname)
    return out

def get_data_from_url_loop(url,start,end,clean_item,product_lists,save_dir_image,limiter,reviewfactor):
    print(clean_item.blacklist)
    futures=range(start,end+1)
    fail=0
    for future in tqdm.tqdm(futures, total=end - start + 1):
        page = future
        try:
            data,fail=get_data_from_url_single(url,page,save_dir_image,fail,limiter,reviewfactor,clean_item) 
            product_lists.append(data)
            R=None
            with open('output/flag.txt', 'r') as f:
                R=f.readline()
            if (R.strip() != "1"):
                break  
            if (fail>3): 
                break
        except Exception as e:
            data = ["get fail"] 
            product_lists.append(data)
    return product_lists

def detect_company(name):
    """ì œí’ˆëª…ì—ì„œ ì œì¡°ì‚¬ ê°ì§€ (ê°„ë‹¨íˆ 'ì‚¼ì„±' ë˜ëŠ” 'ì• í”Œ/APPLE' í¬í•¨ ì—¬ë¶€ë¡œ ë¶„ë¥˜)"""
    if "ì‚¼ì„±" in name:
        return "ì‚¼ì„±"
    elif "ì• í”Œ" in name or "APPLE" in name:
        return "APPLE"
    else:
        return ""

def flatten_reviews(reviews):
    """
    ë¦¬ë·° í•­ëª©ì´ ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° í‰íƒ„í™”í•˜ì—¬ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜  
    (ì˜ê²¬ í…Œì´ë¸”ì— ì €ì¥í•  ë•Œ ì‚¬ìš©)
    """
    flat = []
    if isinstance(reviews, list):
        for item in reviews:
            flat.extend(flatten_reviews(item))
    else:
        flat.append(reviews)
    return flat

def split_by_product(csv_dir):
    """
    CSV íŒŒì¼ë“¤ì„ ì œí’ˆë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    file_types = ['opinions_table.csv', 'product_links.csv']
    
    for file_type in file_types:
        input_path = os.path.join(csv_dir, file_type)
        if not os.path.exists(input_path):
            continue
            
        # CSV íŒŒì¼ ì½ê¸°
        rows_by_product = {}
        with open(input_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                cleaned_name = row['ìƒí’ˆëª…']
                if cleaned_name not in rows_by_product:
                    rows_by_product[cleaned_name] = []
                rows_by_product[cleaned_name].append(row)
        
        # ì œí’ˆë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
        output_dir = os.path.join(csv_dir, f'split_{file_type.split(".")[0]}')
        os.makedirs(output_dir, exist_ok=True)
        
        for product_name, product_rows in rows_by_product.items():
            # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
            safe_name = "".join(c for c in product_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_name = safe_name[:100]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            
            output_path = os.path.join(output_dir, f'{safe_name}.csv')
            
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                if product_rows:
                    writer = csv.DictWriter(f, fieldnames=product_rows[0].keys())
                    writer.writeheader()
                    writer.writerows(product_rows)
def export_custom_csv(pickle_file, output_dir, name_mappings):
    """
    pickle_fileì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ì½ì–´ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        pickle_file: í”¼í´ íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        name_mappings: original_name ë§¤í•‘ ì •ë³´ë¥¼ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸
        [{
            "product_id": "",
            "index": "",
            "original_name": "",
            "name": ""
        }, ...]
    """
    # í”¼í´ íŒŒì¼ ë¡œë“œ
    with open(pickle_file, "rb") as f:
        data = pickle.load(f)
        
    # name_mappingsë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ëœ ì´ë¦„ -> ì›ë³¸ ì´ë¦„)
    name_map = {item["name"]: item["original_name"] for item in name_mappings}
    
    # ê° CSVì— ì €ì¥í•  í–‰ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    product_rows = []   # product_table.csvìš©
    specs_rows = []     # specs_table.csvìš©
    opinions_rows = []  # opinions_table.csvìš©
    links_rows = []     # product_links.csvìš©

    # dataëŠ” ì—¬ëŸ¬ í˜ì´ì§€ì˜ ì œí’ˆ ë¦¬ìŠ¤íŠ¸
    all_products = []
    for page in data:
        all_products.extend(page)
    
    # ì œí’ˆë³„ ìµœëŒ€ ê°€ê²© ê°œìˆ˜ í™•ì¸
    max_price_count = max(len(product.get("price", [])) for product in all_products)
    price_columns = [f"ê°€ê²©{i+1}" for i in range(max_price_count)]
    
    # ê° ì œí’ˆë³„ë¡œ ì²˜ë¦¬
    for product in all_products:
        cleaned_name = product.get("name", "")
        original_name = name_map.get(cleaned_name, cleaned_name)  # ë§¤í•‘ëœ ì›ë³¸ ì´ë¦„ ë˜ëŠ” ëœ ì´ë¦„
        
        # 1. product_table.csvìš© ë°ì´í„° ìƒì„±
        price_list = product.get("price", [])
        prices_formatted = [
            f"{p.get('price', '')} ({p.get('memory_sect', '')})"
            for p in price_list
        ]
        prices_formatted.extend([""] * (max_price_count - len(prices_formatted)))
        
        image_src = product.get("image", {}).get("src", "")
        manufacturer = detect_company(cleaned_name)
        
        # í‰ê· ë³„ì  ê³„ì‚°
        opinion_data = product.get("opinion", {})
        reviews = opinion_data.get("reviews", [])
        stars = []
        for review in reviews:
            if isinstance(review, list) and len(review) == 2:
                star_str = review[1]
                nums = re.findall(r'\d+\.?\d*', star_str)
                if nums:
                    try:
                        stars.append(float(nums[0]))
                    except:
                        pass
        
        average_star = round(sum(stars) / len(stars), 2) if stars else ""
        
        prod_row = {
            "ì›ë³¸ìƒí’ˆëª…": original_name,
            "ìƒí’ˆëª…": cleaned_name
        }
        for idx, col in enumerate(price_columns):
            prod_row[col] = prices_formatted[idx]
        prod_row.update({
            "ì´ë¯¸ì§€src": image_src,
            "ì œì¡°ì‚¬": manufacturer,
            "í‰ê· ë³„ì ": average_star
        })
        product_rows.append(prod_row)
        
        # 2. specs_table.csvìš© ë°ì´í„° ìƒì„±
        specs = product.get("specs", [])
        spec_row = {
            "ì›ë³¸ìƒí’ˆëª…": original_name,
            "ìƒí’ˆëª…": cleaned_name
        }
        for i, spec in enumerate(specs, start=1):
            spec_row[f"ìŠ¤í™{i}"] = spec
        specs_rows.append(spec_row)
        
        # 3. opinions_table.csvìš© ë°ì´í„° ìƒì„±
        for review in reviews:
            if isinstance(review, list) and len(review) == 2:
                opinions_rows.append({
                    "ì›ë³¸ìƒí’ˆëª…": original_name,
                    "ìƒí’ˆëª…": cleaned_name,
                    "ì˜ê²¬": review[0],
                    "star": review[1]
                })
            elif isinstance(review, str):
                opinions_rows.append({
                    "ì›ë³¸ìƒí’ˆëª…": original_name,
                    "ìƒí’ˆëª…": cleaned_name,
                    "ì˜ê²¬": review,
                    "star": ""
                })
        
        # 4. product_links.csvìš© ë°ì´í„° ìƒì„±
        links = product.get("product_link", [])
        for link in links:
            links_rows.append({
                "ì›ë³¸ìƒí’ˆëª…": original_name,
                "ìƒí’ˆëª…": cleaned_name,
                "êµ¬ë§¤ë§í¬": link
            })
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # íŒŒì¼ë“¤ ì €ì¥
    save_csv(os.path.join(output_dir, "product_table.csv"), product_rows,
             ["ì›ë³¸ìƒí’ˆëª…", "ìƒí’ˆëª…"] + price_columns + ["ì´ë¯¸ì§€src", "ì œì¡°ì‚¬", "í‰ê· ë³„ì "])
    
    max_spec_count = max(
        (sum(1 for key in row if key.startswith("ìŠ¤í™")) for row in specs_rows),
        default=0
    )
    save_csv(os.path.join(output_dir, "specs_table.csv"), specs_rows,
             ["ì›ë³¸ìƒí’ˆëª…", "ìƒí’ˆëª…"] + [f"ìŠ¤í™{i}" for i in range(1, max_spec_count+1)])
    
    save_csv(os.path.join(output_dir, "opinions_table.csv"), opinions_rows,
             ["ì›ë³¸ìƒí’ˆëª…", "ìƒí’ˆëª…", "ì˜ê²¬", "star"])
             
    save_csv(os.path.join(output_dir, "product_links.csv"), links_rows,
             ["ì›ë³¸ìƒí’ˆëª…", "ìƒí’ˆëª…", "êµ¬ë§¤ë§í¬"])
    
    # ëª¨ë“  íŒŒì¼ë“¤ì„ ì œí’ˆë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
    split_by_product(output_dir)
    
    print(f"CSV íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_dir}")

def save_csv(filepath, rows, fieldnames):
    """CSV íŒŒì¼ ì €ì¥ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜"""
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            # ë¹ˆ í•„ë“œ ì²˜ë¦¬
            for field in fieldnames:
                if field not in row:
                    row[field] = ""
            writer.writerow(row)




def split_csv_by_product(input_path, output_dir, file_type='opinions', encoding='utf-8'):
    """
    opinions_table.csv ë˜ëŠ” product_links.csvë¥¼ ìƒí’ˆë³„ë¡œ ë¶„í• í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        input_path: CSV íŒŒì¼ ê²½ë¡œ ë˜ëŠ” pickle íŒŒì¼ ê²½ë¡œ
        output_dir: ë¶„í• ëœ íŒŒì¼ë“¤ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
        file_type: 'opinions' ë˜ëŠ” 'links'
        encoding: CSV íŒŒì¼ ì¸ì½”ë”©
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    split_dir = os.path.join(output_dir, f'split_{file_type}')
    os.makedirs(split_dir, exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ
    data = []
    if input_path.endswith('.pkl'):
        with open(input_path, 'rb') as f:
            pickle_data = pickle.load(f)
            # pickle ë°ì´í„°ë¥¼ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if file_type == 'opinions':
                for page in pickle_data:
                    for product in page:
                        original_name = product.get("original_name", "")
                        cleaned_name = product.get("name", "")
                        reviews = product.get("opinion", {}).get("reviews", [])
                        for review in reviews:
                            if isinstance(review, list) and len(review) == 2:
                                data.append({
                                    "ì›ë³¸ìƒí’ˆëª…": original_name,
                                    "ìƒí’ˆëª…": cleaned_name,
                                    "ì˜ê²¬": review[0],
                                    "star": review[1]
                                })
                            elif isinstance(review, str):
                                data.append({
                                    "ì›ë³¸ìƒí’ˆëª…": original_name,
                                    "ìƒí’ˆëª…": cleaned_name,
                                    "ì˜ê²¬": review,
                                    "star": ""
                                })
            else:  # links
                for page in pickle_data:
                    for product in page:
                        original_name = product.get("original_name", "")
                        cleaned_name = product.get("name", "")
                        links = product.get("product_link", [])
                        for link in links:
                            data.append({
                                "ì›ë³¸ìƒí’ˆëª…": original_name,
                                "ìƒí’ˆëª…": cleaned_name,
                                "êµ¬ë§¤ë§í¬": link
                            })
    else:  # CSV íŒŒì¼
        with open(input_path, 'r', encoding=encoding) as f:
            reader = csv.DictReader(f)
            data = list(reader)
    
    # ìƒí’ˆë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
    products = {}
    for row in data:
        key = row['ìƒí’ˆëª…']  # ëœ ì´ë¦„ìœ¼ë¡œ ê·¸ë£¹í™”
        if key not in products:
            products[key] = []
        products[key].append(row)
    
    # ìƒí’ˆë³„ë¡œ íŒŒì¼ ì €ì¥
    for product_name, product_data in products.items():
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        safe_name = "".join(c for c in product_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_name = safe_name[:100]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
        
        output_path = os.path.join(split_dir, f'{safe_name}_{file_type}.csv')
        
        # í•„ë“œ ì´ë¦„ ê²°ì •
        fieldnames = ['ì›ë³¸ìƒí’ˆëª…', 'ìƒí’ˆëª…']
        if file_type == 'opinions':
            fieldnames.extend(['ì˜ê²¬', 'star'])
        else:
            fieldnames.append('êµ¬ë§¤ë§í¬')
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        with open(output_path, 'w', newline='', encoding=encoding) as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(product_data)
    
    return split_dir

def split_all_by_product(input_source, output_dir):
    """
    opinionsì™€ links ë°ì´í„°ë¥¼ ëª¨ë‘ ìƒí’ˆë³„ë¡œ ë¶„í• í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        input_source: pickle íŒŒì¼ ê²½ë¡œ ë˜ëŠ” CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_dir: ë¶„í• ëœ íŒŒì¼ë“¤ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
    """
    if input_source.endswith('.pkl'):
        # pickle íŒŒì¼ì—ì„œ ì§ì ‘ ë¶„í• 
        split_csv_by_product(input_source, output_dir, 'opinions')
        split_csv_by_product(input_source, output_dir, 'links')
    else:
        # CSV íŒŒì¼ë“¤ì—ì„œ ë¶„í• 
        opinions_path = os.path.join(input_source, 'opinions_table.csv')
        links_path = os.path.join(input_source, 'product_links.csv')
        
        if os.path.exists(opinions_path):
            split_csv_by_product(opinions_path, output_dir, 'opinions')
        if os.path.exists(links_path):
            split_csv_by_product(links_path, output_dir, 'links')
    
    print(f"íŒŒì¼ ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì €ì¥ ìœ„ì¹˜: {output_dir}")


def run(clean_item,url="https://prod.danawa.com/list/?cate=22254632",start=1,end=11,output="output"
        ,limiter=100,reviewfactor_in=1):

    limiter=limiter
    
    if (reviewfactor_in == 1): 
        reviewfactor= True
    else: 
        reviewfactor= False
    print(f"reviewfactor: {reviewfactor}")
    # ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” '/'ì™€ null ë¬¸ìê°€ ë¬¸ì œë˜ì§€ë§Œ, '/'ëŠ” ë°˜ë“œì‹œ ì¹˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì— Windowsì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ìë“¤ë„ í•¨ê»˜ ì¹˜í™˜í•˜ë©´ í˜¸í™˜ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.
    safe_url = re.sub(r'[\\/:*?"<>|]', '_', url)
    hashed_filename = hashlib.sha256(url.encode("utf-8")).hexdigest()
    csvname=hashed_filename+".csv"
    # ì•ˆì „í•œ ê²½ë¡œ ìƒì„±
    csv_path = f"{output}/csv"
    pickle_path = f"{output}/pickle"
    image_path = f"{output}/images"
    save_dir_image = Path(image_path) / safe_url
    save_dir_image.mkdir(parents=True, exist_ok=True)
    csv_dir = Path(csv_path)
    pickle_dir = Path(pickle_path)
    pickle_dir.mkdir(parents=True, exist_ok=True)
    csv_dir.mkdir(parents=True, exist_ok=True)
    csv_filename=csv_path+"/"+csvname
    csv_filpath=Path(csv_filename)
    csv_filpath.touch(exist_ok=True)  
    file_path = Path('output/flag.txt')
    file_path.touch()
    
    print(f"ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ: {save_dir_image}")
    print(f"í˜„ì¬ ì›Œí‚¹ ë””ë ‰í† ë¦¬: {os.getcwd()}")

    csv_raw_filename=csv_path+"/"+hashed_filename+"_raw.csv"
    pickle_filename=pickle_path+"/"+hashed_filename+".pickle"
    pickle_output=pickle_path+"/"+hashed_filename+"_output"+".pickle"


    with open('output/flag.txt', 'w') as f:
        f.write("1")
    
    
    product_list=ProductListManager()

    
    data =get_data_from_url_loop(url,start,end,clean_item,product_list,save_dir_image,limiter,reviewfactor)
    clean_item.export_to_csv()
    clean_item.export_raw_data_to_csv(csv_raw_filename)
    extract_name(data)
    data.merge_and_save(pickle_output) 

    export_custom_csv(pickle_output, csv_path,clean_item.export_original_name())
    return True
 
if __name__ == "__main__":
    run()