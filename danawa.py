import os
import re
from pathlib import Path
import tqdm
import pandas as pd
from pathlib import Path
import ast
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import tqdm
import pandas as pd
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from pathlib import Path
import pickle
import os
from pathlib import Path
from difflib import SequenceMatcher
import requests
import csv
import argparse
import hashlib
class SplitTuple(tuple):
    def split(self, sep=None):
        # íŠœí”Œì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì—°ê²°
        joined = ''.join(str(x) for x in self)
        # ë¬¸ìì—´ì„ splití•˜ê³  ê²°ê³¼ë¥¼ ë‹¤ì‹œ SplitTupleë¡œ ë³€í™˜
        return SplitTuple(joined.split(sep))
    
    
def extract_set_from_string(raw_string, keyword_set):
    """
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ `raw_string`ì—ì„œ `keyword_set`ì— í¬í•¨ëœ ëª¨ë“  ë‹¨ì–´ë¥¼ ë¹ ì§ì—†ì´ íƒìƒ‰í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.

    :param raw_string: ê³µë°± ì—†ëŠ” ì…ë ¥ ë¬¸ìì—´
    :param keyword_set: ì°¾ê³ ì í•˜ëŠ” í‚¤ì›Œë“œ ì§‘í•© (set)
    :return: ë§¤ì¹­ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  í‚¤ì›Œë“œë¥¼ ë¹ ì§ì—†ì´ íƒìƒ‰)
    """
    matches = []
    length = len(raw_string)
    
    while raw_string:  # âœ… ë¬¸ìì—´ì´ ë‚¨ì•„ìˆëŠ” ë™ì•ˆ ë°˜ë³µ
        match_found = False
        for size in range(length, 0, -1):  # âœ… ê¸´ í‚¤ì›Œë“œë¶€í„° íƒìƒ‰
            substring = raw_string[:size]  # âœ… ë¬¸ìì—´ì˜ ì²˜ìŒë¶€í„° `size`ë§Œí¼ ê°€ì ¸ì˜¤ê¸°
            if substring in keyword_set:
                matches.append(substring)
                raw_string = raw_string[size:]  # âœ… ë§¤ì¹­ëœ ë¶€ë¶„ ì œê±° í›„ ë‹¤ì‹œ ì²˜ìŒë¶€í„° íƒìƒ‰
                match_found = True
                break  # âœ… ê°€ì¥ ê¸´ ë§¤ì¹­ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ë‹¤ì‹œ ì²˜ìŒë¶€í„° íƒìƒ‰
        if not match_found:
            raw_string = raw_string[1:]  # âœ… ë§¤ì¹­ì´ ì—†ìœ¼ë©´ í•œ ê¸€ìì”© ì´ë™
    
    return matches

def extract_patterns_from_string(raw_string, patterns):
    """
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ íŠ¹ì • ì •ê·œì‹ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ ì¶”ì¶œ.

    :param raw_string: ë„ì–´ì“°ê¸° ì—†ëŠ” ì…ë ¥ ë¬¸ìì—´
    :param patterns: ì°¾ê³ ì í•˜ëŠ” ì •ê·œì‹ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
    :return: íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    """
    matches = []
    index = 0
    length = len(raw_string)
    
    matches = []
    index = 0
    length = len(raw_string)

    while index < length:
        for size in range(length - index, 0, -1):  # ê¸´ í‚¤ì›Œë“œë¶€í„° íƒìƒ‰
            substring = raw_string[index:index+size]
            if any(re.search(pattern, substring) for pattern in patterns):  # âœ… ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ
                matches.append(substring)
                index += size - 1  # âœ… í‚¤ì›Œë“œ ê¸¸ì´ë§Œí¼ ì´ë™
                break
        index += 1  # ë‹¤ìŒ ë¬¸ìë¡œ ì´ë™

    return matches  # âœ… ì°¾ì€ í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜

    #while index < length:
    #    for pattern in patterns:
    #        for size in range(length - index, 0, -1):  # ê¸´ íŒ¨í„´ë¶€í„° ê²€ì‚¬
    #            substring = raw_string[index:index+size]
    #            if re.match(pattern, substring):  # âœ… ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    #                matches.append(substring)
    #                index += size - 1  # âœ… íŒ¨í„´ ê¸¸ì´ë§Œí¼ ì´ë™
    #                break
    #    index += 1  # ë‹¤ìŒ ë¬¸ìë¡œ ì´ë™
    #
    #return matches  # âœ… ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


def extract_year_from_string(raw_string):
    """í•œ ê¸€ìì”© ì´ë™í•˜ë©´ì„œ ì¶œì‹œ ì—°ë„ë¥¼ ê°ì§€"""
    index = 0
    length = len(raw_string)
    detected_years = []

    while index < length - 3:  # ìµœì†Œ 4ìë¦¬ ì—°ë„ ê²€ì‚¬ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        substring = raw_string[index:index+4]
        if re.match(r"^20[2-3][0-9]$", substring):  # âœ… 2020~2029 ë²”ìœ„ ì—°ë„ ê°ì§€
            detected_years.append(substring)
            index += 3  # âœ… ì—°ë„ëŠ” 4ìë¦¬ì´ë¯€ë¡œ, ë‹¤ìŒ íƒìƒ‰ ì¸ë±ìŠ¤ ì¡°ì •
        index += 1
#any(re.match(pattern, substring) for pattern in patterns)
    return list(dict.fromkeys(detected_years))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜





def normalize_product_name(product_name):
    """ë„ì–´ì“°ê¸° ì—†ì´ ì…ë ¥ëœ ë¬¸ìì—´ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ ì •ë¦¬"""
    product_name = convert_none_to_str(product_name)
    
    # âœ… ì‰¼í‘œ ë° `+` ì œê±°
    product_name = product_name.replace(",", " ").replace("+", " ")
    
    # âœ… ê³µë°± ì œê±° í›„ ë¶„ì„ (ë„ì–´ì“°ê¸° ì—†ì´ ë¶™ì–´ ìˆëŠ” ê²½ìš° ëŒ€ë¹„)
    product_name = re.sub(r"\s+", "", product_name)
    
    
    return product_name



def match_score(candidate, reference):
    """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (0~1 ë²”ìœ„)"""
    return SequenceMatcher(None, candidate, reference).ratio()

def extract_keywords_from_string(raw_string, keywords):
    """
    ë„ì–´ì“°ê¸° ì—†ì´ ì…ë ¥ëœ ë¬¸ìì—´ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ ì •ë¦¬.
    - í‚¤ì›Œë“œ ì§‘í•©ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ íƒìƒ‰.
    """
    # âœ… í‚¤ì›Œë“œë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ `set`ì— ì €ì¥
    keyword_set = set()
    for key in keywords:
        keyword_set.update(key.split())  # âœ… ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í‚¤ë¥¼ ë‚˜ëˆ„ì–´ `set`ì— ì €ì¥

    matches = []
    index = 0
    length = len(raw_string)

    while index < length:
        for size in range(length - index, 0, -1):  # ê¸´ í‚¤ì›Œë“œë¶€í„° íƒìƒ‰
            substring = raw_string[index:index+size]
            if substring in keyword_set:  # âœ… ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ
                matches.append(substring)
                index += size - 1  # âœ… í‚¤ì›Œë“œ ê¸¸ì´ë§Œí¼ ì´ë™
                break
        index += 1  # ë‹¤ìŒ ë¬¸ìë¡œ ì´ë™

    return " ".join(matches)  # âœ… ì°¾ì€ í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜

class hashableSlitter(tuple):
    def split(self, sep=None):
        # íŠœí”Œì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì—°ê²°
        joined = ''.join(str(x) for x in self)
        # ë¬¸ìì—´ì„ splití•˜ê³  ê²°ê³¼ë¥¼ ë‹¤ì‹œ SplitTupleë¡œ ë³€í™˜
        return hashableSlitter(joined.split(sep))

def convert_none_to_str(item):
    """Noneì´ë©´ 'None' ë¬¸ìì—´ë¡œ ë³€í™˜, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    return "None" if item is None else item
class ProductDatabasePickleFixed:
    def __init__(self, pickle_filename="product_database.pkl", csv_filename="product_database.csv"):
        self.pickle_filename = pickle_filename  # âœ… Pickle ì €ì¥ íŒŒì¼
        self.csv_filename = csv_filename  # âœ… ë°°í¬ìš© CSV ì €ì¥ íŒŒì¼
        self.current_id = 1  # ID ì´ˆê¸°ê°’
        self.products = {}  # âœ… ì œí’ˆëª… -> ID ë§¤í•‘
        self.blacklist = set()  # âœ… ë¸”ë™ë¦¬ìŠ¤íŠ¸
        self.product_keywords = set()  # âœ… ì œí’ˆ ë¦¬ìŠ¤íŠ¸
        self.company_tags = {"ì‚¼ì„±": "ì‚¼ì„±", "ì• í”Œ": "APPLE", "ì—˜ì§€": "LG", "LG": "LG", "APPLE": "APPLE", "Samsung": "ì‚¼ì„±"}
        self.crawl_index = 1  # âœ… í¬ë¡¤ë§ ì¸ë±ìŠ¤ ì´ˆê¸°ê°’
        self.raw_data = {}  # âœ… ID -> ì›ë³¸ ì´ë¦„ ë°ì´í„° ë§¤í•‘
        
        # âœ… Pickle íŒŒì¼ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±)
        self.load_or_create_pickle()
    def export_raw_data_to_csv(self, file_path):
        """ê²½ë¡œ(file_path)ë¥¼ ì…ë ¥ë°›ì•„ self.raw_dataë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë©”ì„œë“œ.
        
        self.raw_dataëŠ” {product_id: [ { "index":..., "original_name":..., "name":... }, ... ], ... } í˜•íƒœì…ë‹ˆë‹¤.
        ê° í–‰ì—ëŠ” product_id, index, original_name, name í•­ëª©ì´ í¬í•¨ë©ë‹ˆë‹¤.
        """
        import csv
        with open(file_path, "w", newline="", encoding="utf-8") as csvfile:
            fieldnames = ["product_id", "index", "original_name", "name"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for product_id, records in self.raw_data.items():
                for record in records:
                    # ê° recordëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. ì—¬ê¸°ì— product_idë¥¼ ì¶”ê°€í•˜ì—¬ í•œ í–‰(row)ë¡œ ë§Œë“­ë‹ˆë‹¤.
                    row = {
                        "product_id": product_id,
                        "index": record.get("index", ""),
                        "original_name": record.get("original_name", ""),
                        "name": record.get("name", "")
                    }
                    writer.writerow(row)
        print(f"âœ… CSV íŒŒì¼ `{file_path}` ì €ì¥ ì™„ë£Œ!")
    def load_or_create_pickle(self):
        hardcoded_blacklist = {
            "Cellular", "í•´ì™¸êµ¬ë§¤", "ì¤‘ê³ ", "Cíƒ€ì…", "5G", "LTE", "í‚¤ë³´ë“œ", "ë¶ì»¤ë²„", "ì¼€ì´ìŠ¤",
            "ë§¤ì§í‚¤ë³´ë“œ", "í´ë¦¬ì˜¤ì¼€ì´ìŠ¤", "ì• í”ŒíœìŠ¬", "í´ë¦¬ì˜¤í‚¤ë³´ë“œ", "í´ë¦¬ì˜¤ì¼€ì´ìŠ¤","í´ë¦¬ì˜¤í‚¤ë³´ë“œ","ìê¸‰ì œ"
        }
            # âœ… ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ê¸°ë³¸ê°’
        hardcoded_product_keywords = {
        "iPad", "ê°¤ëŸ­ì‹œíƒ­", "ê°¤ëŸ­ì‹œZ","Surface", "GíŒ¨ë“œ", "Yoga", "Xperia", "MatePad","ê°¤ëŸ­ì‹œ","ìš¸íŠ¸ë¼","í”Œë¦½","í´ë“œ"
        "ëƒ‰ì¥ê³ ", "ì„¸íƒê¸°", "TV", "ì˜¤ë¸", "ì „ìë ˆì¸ì§€", "ì—ì–´ì»¨", "ê³µê¸°ì²­ì •ê¸°", "iPhone","ì•„ì´í°","ë§¥ë¶","ë§¥ë¯¸ë‹ˆ","ë§¥ë¶ì—ì–´","ë§¥ë¶í”„ë¡œ",
        "ì²­ì†Œê¸°", "ìŠ¤í”¼ì»¤", "ëª¨ë‹ˆí„°", "M1", "M2", "M3", "M4", "M5", "mini", "Air","í”ŒëŸ¬ìŠ¤"
        }
        hardcoded_product = {
            "APPLE 2021 iPad Pro 11 3ì„¸ëŒ€": 1,
            "APPLE 2021 iPad Pro 12.9 5ì„¸ëŒ€": 2,
            "APPLE 2022 iPad Air 5ì„¸ëŒ€": 3,
            "APPLE 2022 iPad Pro 11 4ì„¸ëŒ€": 4,
            "APPLE 2022 iPad Pro 12.9 6ì„¸ëŒ€": 5,
            "APPLE 2024 iPad Air 13 M2": 6,
            "APPLE 2024 iPad Air 13 M2 í”„ë¡œ": 7,
            "APPLE 2024 iPad mini A17 Pro 7ì„¸ëŒ€": 8,
            "APPLE 2024 iPad Pro 11 M4": 9,
            "APPLE 2024 iPad Pro 11 M4 í”„ë¡œ": 10,
            "APPLE 2024 iPad Pro 13 M4": 11,
            "APPLE 2024 iPad Pro 13 M4 í”„ë¡œ": 12,
            "ë ˆë…¸ë²„ ìš”ê°€ Pad Pro AI": 13,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S10 ìš¸íŠ¸ë¼": 14,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S10 í”ŒëŸ¬ìŠ¤": 15,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S8": 16,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S8 ìš¸íŠ¸ë¼": 17,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S8 í”ŒëŸ¬ìŠ¤": 18,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S9": 19,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S9 ìš¸íŠ¸ë¼": 20,
            "ì‚¼ì„±ì „ì ê°¤ëŸ­ì‹œíƒ­ S9 í”ŒëŸ¬ìŠ¤": 21,
            "ìƒ¤ì˜¤ë¯¸ ë¯¸ íŒ¨ë“œ7": 22,
            "ìƒ¤ì˜¤ë¯¸ ë¯¸ íŒ¨ë“œ7 í”„ë¡œ": 23
                }

        """Pickle íŒŒì¼ì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±"""

        if not os.path.exists(self.pickle_filename):
            self.blacklist = hardcoded_blacklist  # âœ… ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í•˜ë“œì½”ë”© ê°’ ì¶”ê°€
            print(f"âš ï¸ Pickle íŒŒì¼ `{self.pickle_filename}`ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            self.product_keywords = hardcoded_product_keywords
            self.products = hardcoded_product
            self.current_id = 24 
            self.save_to_pickle()  # âœ… ê¸°ë³¸ê°’ìœ¼ë¡œ íŒŒì¼ ìƒì„±
        else:
            with open(self.pickle_filename, "rb") as f:
                data = pickle.load(f)
                self.products = data.get("products", {})
                self.blacklist = data.get("blacklist", set())
                self.product_keywords = data.get("product_keywords", set())
                if isinstance(self.products, set):
                    print("âŒ ì˜¤ë¥˜: productsê°€ `set`ìœ¼ë¡œ ì €ì¥ë¨! ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                    self.products = hardcoded_product  # âœ… ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
                    self.current_id = 23
                self.current_id = max(self.products.values(), default=1) + 1  # âœ… ID ìë™ ì¦ê°€

    def save_to_pickle(self):
        """í˜„ì¬ ë°ì´í„°ë¥¼ Pickle íŒŒì¼ì— ì €ì¥"""
        with open(self.pickle_filename, "wb") as f:
            pickle.dump({
                "products": self.products,
                "blacklist": self.blacklist,
                "product_keywords": self.product_keywords
            }, f)
        print(f"âœ… Pickle íŒŒì¼ `{self.pickle_filename}` ì €ì¥ ì™„ë£Œ!")

    def detect_company(self, product_name):
        """ì œí’ˆëª…ì—ì„œ íšŒì‚¬ íƒœê·¸ë¥¼ ê°ì§€í•˜ì—¬ ìë™ ë¶„ë¥˜"""
        for tag, company in self.company_tags.items():
            if tag in product_name:
                return company
        return "ê¸°íƒ€"



    def filter_and_standardize(self, product_name):
        """ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° í›„, ê°€ì¥ ì í•©í•œ ë‹¨ì–´ ìˆœì„œë¡œ ì •ë¦¬í•˜ì—¬ ëª¨ë¸ëª…ì„ ìƒì„±"""
        original_name = convert_none_to_str(product_name)  # âœ… ì›ë³¸ ì œí’ˆëª… ì €ì¥
        product_name = normalize_product_name(original_name)  # âœ… ê³µë°± ì œê±° í›„ ì •ë¦¬
        for word in self.blacklist:
            product_name = product_name.replace(word, " ")
            product_name = re.sub(r"\s+", " ", product_name).strip()
        extracted_name = extract_keywords_from_string(product_name, self.products.keys())
        words = extracted_name.split()

        # âœ… ì¶œì‹œ ì—°ë„ ê°ì§€ (ì˜ˆ: 2022, 2023, 2024 ë“±)
        year_match = extract_patterns_from_string(product_name, [r"^20[2-3][0-9]$"])
        #[word for word in words if re.match(r"^20[2-3][0-9]$", word)]
        release_year = year_match[0] if year_match else ""

        # âœ… ëª¨ë¸ ë„˜ë²„ ê°ì§€ (ì˜ˆ: "11", "12.9", "S8", "M2", "M4", "6ì„¸ëŒ€" ë“±)
        model_numbers = extract_patterns_from_string(product_name, [r"^(S\d{1,2}|M\d{1,2}|\d+ì„¸ëŒ€|\d{2,4}GB|\d{1,3}(\.\d+)?GHz|\d{1,2}A|\Ad{1,2}|í´ë“œ\d{1})$"])
        keyword_member = extract_set_from_string(product_name, self.product_keywords)
        #[word for word in words if re.match(r"^(\d+(\.\d+)?|S\d+|M\d+|\d+ì„¸ëŒ€+Air)$", word)]
        # âœ… í•˜ë“œì½”ë”©ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ì™€ ë¹„êµí•˜ì—¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì°¾ê¸°
        common_words_set = set()
        for model in self.products:
            common_words = set(words) & set(model.split())  # âœ… ì œí’ˆëª…ê³¼ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì¶”ì¶œ
            if common_words:
                common_words_set = common_words
                break  # âœ… í•œ ë²ˆì´ë¼ë„ ê²¹ì¹˜ë©´ ë°”ë¡œ ì‚¬ìš©
        # âœ… ë“±ë¡ë˜ì§€ ì•Šì€ ë¬¸êµ¬ë„ ì¶”ê°€ë¡œ ë°˜ì˜

        # âœ… ì œì¡°ì‚¬ ê°ì§€ (ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€)
        detected_product = self.detect_company(original_name)
        # âœ… ê°€ì¥ ì í•©í•œ ì œí’ˆëª… ì°¾ê¸°
        total_word=[]
        total_word.append(detected_product)
        for i in common_words:
            total_word.append(i)
        for i in model_numbers:
            total_word.append(i)
        total_word.append(release_year)
        total_word.extend(keyword_member)
        unregistered_word=product_name
        for word in total_word:
            unregistered_word = unregistered_word.replace(word, " ")    
        unregistered_words =[word for word in unregistered_word.split() if len(word) >21] 
        total_word.extend(unregistered_words)
        total_word = ["ì‚¼ì„±ì „ì" if word == "ì‚¼ì„±" else word for word in total_word]
        common_words_set = set(total_word)
        best_match = self.match_best_product(common_words_set,original_name)
        # âœ… ìµœì¢… ëª¨ë¸ëª… ìƒì„± (ì œì¡°ì‚¬ + ì •ë ¬ëœ ëª¨ë¸ëª… + ëª¨ë¸ ë„˜ë²„ë§ + ì¶œì‹œ ì—°ë„)
        print(f" before join âœ… ì •ì œëœ ì œí’ˆëª…: {best_match}, ì›ë³¸: {original_name}, Process Number: {self.current_id}")
        # âœ… ê¸°ì¡´ ì œí’ˆëª…ì´ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ì¡°í•© ìƒì„±
        standardized_name = ' '.join(best_match).strip() if best_match else f"{' '.join(common_words_set)}".strip()

        print(f"âœ… ì •ì œëœ ì œí’ˆëª…: {standardized_name}, ì›ë³¸: {original_name}, Process Number: {self.current_id}")
        return original_name,standardized_name   # âœ… ì›ë³¸, ì •í™”ëœ ì œí’ˆëª… í•¨ê»˜ ë°˜í™˜

    def add_product(self, product_name):
        """ìƒˆë¡œìš´ ì œí’ˆì„ ì¶”ê°€í•˜ë©´ì„œ ì¸¡ì • ì¸ë±ìŠ¤ + ì›ë³¸ ì œí’ˆ ID + ì˜¤ì—¼ ëª¨ë¸ëª…ì„ í•¨ê»˜ ì €ì¥"""
        original_name, standardized_product = self.filter_and_standardize(product_name)  # âœ… ì›ë³¸ & ì •ì œëœ ì œí’ˆëª…
        
        # âœ… ê¸°ì¡´ ì œí’ˆì´ë©´ ê¸°ì¡´ ID ìœ ì§€ (í•˜ì§€ë§Œ original_nameë„ ì €ì¥í•´ì•¼ í•¨!)
        if standardized_product in self.products:
            assigned_id = self.products[standardized_product]
        else:
            # âœ… ìƒˆë¡œìš´ ì œí’ˆì´ë©´ ìƒˆë¡œìš´ ID í• ë‹¹
            assigned_id = self.current_id
            self.products[standardized_product] = assigned_id
            self.current_id += 1  # âœ… ìƒˆë¡œìš´ ì œí’ˆì´ ì¶”ê°€ë  ë•Œë§Œ ì¦ê°€!

        # âœ… ì›ë³¸ ë°ì´í„° ì €ì¥ (ê¸°ì¡´ ì œí’ˆì´ë“  ìƒˆë¡œìš´ ì œí’ˆì´ë“  ì €ì¥!)
        if assigned_id not in self.raw_data:
            self.raw_data[assigned_id] = []  # âœ… IDë³„ë¡œ ì—¬ëŸ¬ ì›ë³¸ëª…ì„ ì €ì¥ ê°€ëŠ¥í•˜ë„ë¡ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€ê²½

        self.raw_data[assigned_id].append({
            "index": self.crawl_index,
            "original_name": original_name,
            "name": standardized_product
        })
        self.crawl_index += 1
        # âœ… Pickleì— ì €ì¥
        self.save_to_pickle()
    def match_best_product(self, sorted_product,original_order):
        """
        ì›ë³¸ ì œí’ˆëª…(`original_order`)ì˜ ë‹¨ì–´ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ `sorted_product`ì˜ ë‹¨ì–´ë“¤ì„ ì¬ë°°ì—´.

        :param original_order: ì›ë³¸ ì œí’ˆëª… ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        :param sorted_product: ì •ë ¬í•´ì•¼ í•  ë‹¨ì–´ ì§‘í•© (set)
        :return: ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        """
        original_order = original_order.replace(" ", "")  # âœ… ê³µë°± ì œê±° í›„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°
        matched_words = []  # âœ… ë§¤ì¹­ëœ ë‹¨ì–´ ì €ì¥
        remaining_string = original_order  # âœ… íƒìƒ‰í•  ë¬¸ìì—´ ì´ˆê¸°í™”
        match_found = False
        while remaining_string:  # âœ… ë¬¸ìì—´ì´ ë‚¨ì•„ìˆëŠ” ë™ì•ˆ ë°˜ë³µ
            for word in sorted_product:
                if not word:
                    continue
                if (word==remaining_string[0:len(word)]) and (word not in matched_words):  # âœ… ë‹¨ì–´ê°€ ì¡´ì¬í•˜ë©´ ë°˜ë³µì ìœ¼ë¡œ ì œê±°
                    matched_words.append(word)  # âœ… ë§¤ì¹­ëœ ë‹¨ì–´ ì €ì¥
                    remaining_string = remaining_string.replace(word, "", 1)
                    match_found = True
            if not match_found:
                remaining_string = remaining_string[1:]
            else:
                match_found = False        
                     
           

        return matched_words
        #best_match = None
        #highest_score = 0

        
        
        # âœ… ìƒˆë¡œìš´ ì œí’ˆëª…ì˜ ë‹¨ì–´ë¥¼ ì •ë ¬í•˜ì—¬ ë¹„êµ
        #sorted_new_product = " ".join(sorted(new_product_set))
        
        #for existing_product in self.products.keys():
        #    sorted_existing_product = " ".join(sorted(existing_product.split()))
        #    similarity = SequenceMatcher(None, sorted_new_product, sorted_existing_product).ratio()
#
        #    if similarity > highest_score:
        #        highest_score = similarity
        #        best_match = existing_product
        #return best_match if highest_score > 0.9 else None  # âœ… ìœ ì‚¬ë„ê°€ 90% ì´ìƒì´ë©´ ë§¤ì¹­ ì„±ê³µ
        return sorted_product_out

    def export_to_csv(self):
        """í˜„ì¬ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (ë°°í¬ìš©)"""
        import pandas as pd
        df = pd.DataFrame(self.products.items(), columns=["Standardized_Product", "ID"])
        df.to_csv(self.csv_filename, index=False)
        print(f"âœ… CSV íŒŒì¼ `{self.csv_filename}` ì €ì¥ ì™„ë£Œ!")

    def add_to_blacklist(self, item):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ë‹¨ì–´ ì¶”ê°€"""
        self.blacklist.add(item)
        self.save_to_pickle()

    def remove_from_blacklist(self, item):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì œê±°"""
        if item in self.blacklist:
            self.blacklist.remove(item)
            self.save_to_pickle()

    def read_blacklist(self):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        return list(self.blacklist)

    def add_to_product_list(self, item):
        """ì œí’ˆêµ° ë¦¬ìŠ¤íŠ¸ì— ë‹¨ì–´ ì¶”ê°€"""
        self.product_keywords.add(item)
        self.save_to_pickle()

    def remove_from_product_list(self, item):
        """ì œí’ˆêµ° ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì œê±°"""
        if item in self.product_keywords:
            self.product_keywords.remove(item)
            self.save_to_pickle()

    def read_product_list(self):
        """ì œí’ˆêµ° ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        return list(self.product_keywords)

class TapName:
    def __init__(self, driver):
        self.opinion = "productOpinion"
        self.companyReview = "companyReview"
        self.mcReview = "mcReview"
        self.front = "danawa-prodBlog-"
        self.mid = "-button-tab-"
        self.driver = driver
        self.review=[]
        self.average_star=None
        #review Parsing parm name
        self.second_selector = 'div.common_paginate'
        self.third_selector = 'div.nums_area'
    def getfirst(self):
        html = self.driver.page_source
        try:
            soup = BeautifulSoup(html, 'html.parser')
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
            tab.click()
        except:
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.companyReview+self.mid+self.opinion)))
            tab.click()
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            tab=WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.ID, self.front+self.opinion+self.mid+self.companyReview)))
            tab.click()
            
        result=self.extract_reviews_general(soup)
        self.get_avg_star()
        return result
    def get_avg_star(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        selector="#danawa-prodBlog-productOpinion-list-self > div.mall_review > div.area_left > div.grade_area > div.grd_stats > div.point_num > strong"
        elems = soup.select(selector)
        if elems:
            self.average_star = elems[0].get_text(strip=True)
        else:
            self.average_star = "no review"
        self.get_second()
    
    def get_second(self):
        html = self.driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        result=self.extract_reviews_general(soup)
        return result
    def oneshot(self):
        self.review.append(self.getfirst())
        self.review.extend(self.get_second())
        return self.review
    def clean(self):
        self.driver.quit()
        return self.review
    def oneshot_iter(self):
        out=[]
        try:
            out.extend(self.oneshot())
        except:
            try:
                flag=True
                n=2
                while flag:
                    flag=self.click_opinion_page(n)
                    if flag:
                        out.extend(self.get_second())
                        n=n+1                   
            except:
                if out:
                    return out
                else:
                    return out.append(["no review"])
        try:
            flag=True
            n=2
            while flag:
                flag=self.click_opinion_page(n)
                if flag:
                    out.extend(self.get_second())
                    n=n+1                   
        except:
            if out:
                return out
            else:
                return out.append(["no review"])
        return out    

    def extract_reviews_general(self, soup, similarity_threshold=0.7):
        try:
            reviews = []
            candidate_selectors = [
                'p.danawa-prodBlog-productOpinion-list-self',
                ("div", {"id": "danawa-prodBlog-productOpinion-list-self"}),
                ('div', {"id": "danawa-prodBlog-companyReview-content-list"}),
            ]
            # í›„ë³´ ì…€ë ‰í„°ë“¤ë¡œë¶€í„° ë¦¬ë·° ìš”ì†Œ(filtered_elems) ìˆ˜ì§‘
            for selector in candidate_selectors:
                if isinstance(selector, str):
                    elems = soup.select(selector)
                else:
                    elems = soup.find_all(*selector)
                if elems:
                    filtered_elems = []
                    for elem in elems:
                        parent_div = elem.find_all('li', class_="danawa-prodBlog-companyReview-clazz-more")
                        if parent_div:
                            filtered_elems.extend(parent_div)
                        else:
                            parent_div = elem.find_all(
                                'p',
                                class_="danawa-prodBlog-productOpinion-clazz-content",
                                attrs={"data-seq": "N"}
                            )
                            if parent_div:
                                filtered_elems.extend(parent_div)
            # ê° ë¦¬ë·° ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸(ë° ë³„ì ) ì¶”ì¶œ
            for felem in filtered_elems:
                text_elements = felem.find_all('div', class_="atc")
                if self.average_star:
                    star = felem.find_all('span', class_="star_mask")
                    if text_elements:
                        for elem in text_elements:
                            if hasattr(elem, "get_text"):
                                cleaned = " ".join(set(elem.get_text(separator=' ', strip=True).split()))
                            else:
                                cleaned = " ".join(set(str(elem).split()))
                            if cleaned:
                                # ë³„ì ì´ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶”ê°€
                                if star and len(star) > 0:
                                    reviews.append([cleaned, star[0].get_text(strip=True)])
                                else:
                                    reviews.append(cleaned)
                if text_elements:
                    for elem in text_elements:
                        if hasattr(elem, "get_text"):
                            cleaned = " ".join(set(elem.get_text(separator=' ', strip=True).split()))
                        else:
                            cleaned = " ".join(set(str(elem).split()))
                        if cleaned:
                            reviews.append(cleaned)
                else:
                    if hasattr(felem, "get_text"):
                        cleaned = " ".join(set(felem.get_text(separator=' ', strip=True).split()))
                    else:
                        cleaned = " ".join(set(str(felem).split()))
                    if cleaned:
                        reviews.append(cleaned)
            # ì¤‘ë³µ ì œê±° (Jaccard ìœ ì‚¬ë„ ê¸°ì¤€)
            unique_reviews = []
            for review in reviews:
                is_duplicate = False
                for unique_review in unique_reviews:
                    if jaccard_similarity(review, unique_review) > similarity_threshold:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    unique_reviews.append(review)
            return unique_reviews
        except Exception as e:
            print("error", e)
            return unique_reviews
    def click_opinion_page(self, page):
        try:
            # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (ìˆ˜ì •ë¨: CSS Selectorë¡œ ë³€ê²½)
            first = self.driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-companyReview-content-list")
            if not first:
                return False

                
            # âœ… ë‘ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.second_selector ì‚¬ìš©)
            Spagr = self.driver.find_elements(By.CSS_SELECTOR, self.second_selector)
            if not Spagr:
                return False
            if page%10==1:
                try:
                    page_next_buttons = WebDriverWait(self.driver, 10).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, "a.nav_edge_next.nav_edge_on"))
                    )
                    page_next_buttons.click()
                    return True
                except:
                    return False
             #âœ… ì„¸ ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (self.third_selector ì‚¬ìš©)
            Tpagr = self.driver.find_elements(By.CSS_SELECTOR, self.third_selector)
            if not Tpagr:
                return False
            #if page%10==0:
                #try:
                #    page_last_buttons = WebDriverWait(self.driver, 10).until(
                #            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#danawa-prodBlog-companyReview-content-list > div > div > div > span"))
                #                )
                #    page_last_buttons[0].click()
                #    return True
                #except:
                #    return False
            # âœ… í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° (WebDriverWait ì ìš©)
            page_buttons = WebDriverWait(self.driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.page_num[data-pagenumber]"))
            )
            # âœ… íŠ¹ì • í˜ì´ì§€ ë²ˆí˜¸ í´ë¦­ (ë¬¸ìì—´ ë³€í™˜ ì¶”ê°€)
            for button in page_buttons:
                page_number = button.get_attribute("data-pagenumber")
                if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                    self.driver.execute_script("arguments[0].click();", button)
                    return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜    
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
            return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜
        return False  # í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ì„ ê²½ìš° False
 
def review_loop (url,trynum=5):
    options = Options()
    options.add_argument('--headless') 
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument("--disable-dev-shm-usage")  # âœ… `/dev/shm` ë¶€ì¡± ë¬¸ì œ í•´ê²°
    options.add_argument("--remote-debugging-port=9222")  # âœ… ë””ë²„ê¹… í™œì„±í™”
    driver = webdriver.Chrome(service = Service(),options=options)
    # í¬ë¡¤ë§í•  í˜ì´ì§€ URL (ì‹¤ì œ URLë¡œ ë³€ê²½)
    if len(url)<5:
        print("url")
        return ["no review"]
    driver.get("https://prod.danawa.com/"+url)
    # ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•´ ì¶©ë¶„í•œ ì‹œê°„ ëŒ€ê¸° (WebDriverWait ì‚¬ìš© ê¶Œì¥)
    WebDriverWait(driver, 5).until(lambda driver: driver.execute_script("return document.readyState") == "complete")
    # ë‘ ê°œì˜ ì˜ê²¬/ë¦¬ë·° íƒ­ ID ëª©ë¡ (í•„ìš”í•œ ê²½ìš° ë‘ íƒ­ ëª¨ë‘ ì˜ê²¬/ë¦¬ë·° ë²”ì£¼ì— í•´ë‹¹í•œë‹¤ê³  ê°€ì •)
    desired_tab_ids = {
        "danawa-prodBlog-productOpinion-button-tab-productOpinion",
        "danawa-prodBlog-productOpinion-button-tab-companyReview"
    }
    # í˜„ì¬ í™œì„± íƒ­ì˜ <a> íƒœê·¸ ì„ íƒ (ìƒìœ„ íƒ­ ì˜ì—­ì—ì„œ í™œì„± íƒ­ì€ li ìš”ì†Œì— "on" í´ë˜ìŠ¤ê°€ ë¶™ì–´ ìˆìŠµë‹ˆë‹¤)
    active_tab_elements = WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.tab_item"))
    )
    #driver.find_elements(By.CSS_SELECTOR, "#danawa-prodBlog-productOpinion-button-tab-productOpinion")
    for active_tab_element in active_tab_elements:
        classname = active_tab_element.get_attribute("class")
        if classname.endswith("on"):
            if not 'bookmark_cm_opinion_item'==active_tab_element.get_attribute("id"):
                        T=0
                        while T<trynum:
                            try:
                                product_opinion_tab = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "#danawa-prodBlog-productOpinion-button-tab-productOpinion")))
                                product_opinion_tab.click(),
                                break
                            except:
                                T=T+1
                                print("retry")
                        if T==trynum:
                            print("fail")
                            return False
            else:
                break
    # ì›í•˜ëŠ” ì‹œì‘ ë…¸ë“œë¥¼ ì§€ì •í•œ í›„, íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    html = driver.page_source  # Seleniumì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ HTML
    soup = BeautifulSoup(html, 'html.parser')
    try:
        reviews=TapName(driver)
        out=reviews.oneshot_iter()
        driver.quit()
        return out
    except:
        print("fail resource release")
        return reviews.clean()


def jaccard_similarity(text_1, text_2):
    """ ë‘ ê°œì˜ ë¬¸ì¥ ê°„ Jaccard Similarity ê³„ì‚° """
    if isinstance(text_1, list):
        text1 = text_1[0]
    else:
        text1 = text_1
    if isinstance(text_2, list):
        text2 = text_2[0]
    else:
        text2 = text_2
    set1, set2 = set(text1.split()), set(text2.split())
    intersection = len(set1 & set2)  # ê³µí†µ ë‹¨ì–´ ê°œìˆ˜
    union = len(set1 | set2)  # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜
    return intersection / union if union != 0 else 0

def click_page(page,driver,timeout=10):
    try:
        # âœ… ì²« ë²ˆì§¸ ì„ íƒì ê²€ì‚¬ (í˜ì´ì§€ ì½˜í…ì¸ ê°€ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#productListArea"))
        )
        if page%10==1:
            try:
                next_page_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "#productListArea div.prod_num_nav a.edge_nav.nav_next"))
                )
                next_page_button.click()
                return True
            except:
                try:
                    driver.execute_script("arguments[0].click();", next_page_button)
                    return True
                except:
                    return False
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.number_wrap"))
        )
        page_buttons = driver.find_elements(By.CSS_SELECTOR, "div.number_wrap a.num")
        for button in page_buttons:
            page_number = button.text.strip()
            if str(page_number) == str(page):  # ğŸ”¹ ë¹„êµ ì˜¤ë¥˜ í•´ê²°
                driver.execute_script("arguments[0].click();", button)
                WebDriverWait(driver, timeout).until(EC.staleness_of(button))
                return True  # âœ… í´ë¦­ ì„±ê³µ ì‹œ True ë°˜í™˜
        return False  # âœ… í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ì„ ê²½ìš° False 
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")  # âœ… ë””ë²„ê¹…ì„ ìœ„í•´ ì˜¤ë¥˜ ì¶œë ¥
        return False  # ì‹¤íŒ¨ ì‹œ False ë°˜í™˜



def get_data_from_url_single(url,num,save_dir_image,fail,limmiter,reviewfactor):
    options = Options()
    
    #options.add_argument('--headless')      
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(service = Service(),options=options)
    product_list=[]
    try:
        # âœ… í˜ì´ì§€ ì´ë™
        driver.get(url)
        # âœ… ì²« ë²ˆì§¸ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")
        # âœ… í˜ì´ì§€ ì´ë™ (num í˜ì´ì§€ í´ë¦­)
        if num != 1:
            count=click_page(num, driver)  
            if not count:
                fail+=1
            WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")
        html = driver.page_source
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'html.parser')
        # ì œí’ˆ ì •ë³´ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì—¬ëŸ¬ ìƒí’ˆì´ ìˆì„ ê²½ìš° ë°˜ë³µë¬¸ ì‚¬ìš©)
        all_product_divs = soup.find_all('div', class_='prod_main_info')
        # ì—¬ëŸ¬ ìƒí’ˆ ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        debug=0
        for prod_info_div in all_product_divs:
            product_info = {}
            if prod_info_div:
                # ì œí’ˆëª… ì¶”ì¶œ: <p class="prod_name"> ë‚´ë¶€ì˜ <a name="productName"> íƒœê·¸
                prod_name_tag = prod_info_div.find('p', class_='prod_name')
                if prod_name_tag:
                    a_tag = prod_name_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = a_tag.get_text(strip=True)
                prod_op_link_tag = prod_info_div.find('p', class_='prod_name')
                if prod_op_link_tag:
                    op_link_tag = prod_op_link_tag.find('a', attrs={"name": "productName"})
                    if a_tag:
                        product_info['name'] = op_link_tag.get_text(strip=True)
                comment_div = prod_info_div.find('div', class_='meta_item mt_comment')
                if comment_div:
                    a_comment = comment_div.find('a')
                    if a_comment:
                        # ì œí’ˆì˜ê²¬ ê´€ë ¨ ì •ë³´: ë§í¬, ì˜ê²¬ ìˆ˜ ë“±
                        product_info['opinion'] = {} 
                        product_info['opinion']['link'] = a_comment.get('href')
                        strong_tag = a_comment.find('strong')
                        if strong_tag:
                            product_info['opinion']['count'] = strong_tag.get_text(strip=True)
                # ìŠ¤í™ ì˜ì—­ ì¶”ì¶œ: <div class="spec_list"> ë‚´ë¶€ì˜ ëª¨ë“  ìŠ¤í™ ì •ë³´ ì¶”ì¶œ
                spec_items = []
                spec_div = prod_info_div.find('div', class_='spec_list')
                if spec_div:
                    # <a class="view_dic"> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ì¶”ì¶œ
                    for a in spec_div.find_all('a', class_='view_dic'):
                        if a:
                            text = a.get_text(strip=True)
                            spec_items.append(text)
                ##NEXT IMAGEFUN  save_dir_image
                image_tag = prod_info_div.find('img')
                if image_tag:
                    image_src = image_tag.get('src')
                    download_image(image_src, save_dir_image, product_info['name'])
                    product_info['image'] = {
                            'src': image_src,
                            'saved_path': os.path.join(save_dir_image, product_info['name'])
                        }
                price_info=extract_prod_info_list(prod_info_div)
                #download_image(ëª©í‘œì£¼ì†Œ, save_dir_image, ì´ë¦„)
                product_info['price'] = price_info
                # ì¶”ì¶œí•œ ìŠ¤í™ ë°°ì—´ì„ ì œí’ˆ ì •ë³´ì— ì¶”ê°€ (ë°°ì—´ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê±°ë‚˜, join()ìœ¼ë¡œ ë¬¸ìì—´ ê²°í•© ê°€ëŠ¥)
                product_info['specs'] = spec_items
                print (f"debug print : {product_info['name']}")
                clean_itam.add_product(product_info['name'])
                product_list.append(product_info)
            if reviewfactor:    
                for i in range(0, len(product_list)):
                    if 'opinion' in product_list[i]:
                        try:
                            product_list[i]['opinion']["reviews"] =review_loop(product_list[i]['opinion']['link'])
                        except:
                            product_list[i]['opinion']["reviews"] = ["no review"]
            debug+=1
            if debug>=limmiter:
                break       
                        
        return product_list, fail
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ maim loop: {e}")
        if not product_list:
            product_list = ["get fail"]
        return product_list,fail
    finally:
        driver.quit()  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¸Œë¼ìš°ì €ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
def extract_prod_info_list(soup):
    """
    idê°€ "productInfoDetail_"ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  <li> ìš”ì†Œ ë‚´ë¶€ì—ì„œ,
      - <p> íƒœê·¸ì˜ classë¥¼ í‚¤ë¡œ, ê·¸ ë‚´ìš©ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” í•­ëª©ë“¤ì„ ì¶”ì¶œí•˜ê³ ,
      - ë³„ë„ë¡œ ê°€ê²© ì •ë³´(p.price_sect > a > strong)ë¥¼ "price" í‚¤ì— ì €ì¥í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    ê° <li> ë³„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    result_list = []
    
    # idê°€ "productInfoDetail_"ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  <li> ìš”ì†Œ ì°¾ê¸°
    li_elements = soup.select('li[id^="productInfoDetail_"]')
    
    for li in li_elements:
        product_dict = {}
        
        # li ë‚´ë¶€ì˜ ëª¨ë“  <p> íƒœê·¸ ìˆœíšŒ
        p_tags = li.find_all('p')
        for p in p_tags:
            # p íƒœê·¸ì— class ì†ì„±ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
            p_classes = p.get("class")
            if not p_classes:
                continue
            
            # ê°€ê²© ì •ë³´ìš© p íƒœê·¸("price_sect")ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
            if "price_sect" in p_classes:
                continue
            
            # p íƒœê·¸ì˜ class ëª©ë¡ì„ ê³µë°±ìœ¼ë¡œ í•©ì³ keyë¡œ ì‚¬ìš©
            key = " ".join(p_classes)
            # p íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸(ìì‹ íƒœê·¸ í¬í•¨)ë¥¼ valueë¡œ ì‚¬ìš©
            value = p.get_text(separator=" ", strip=True)
            
            product_dict[key] = value
        
        # ê°€ê²© ì •ë³´ ì¶”ì¶œ: li ë‚´ë¶€ì˜ 'p.price_sect > a > strong'
        price_tag = li.select_one("p.price_sect a strong")
        if price_tag:
            product_dict["price"] = price_tag.get_text(strip=True)
        
        result_list.append(product_dict)
    
    return result_list
def download_image(image_url, save_dir, filename=None):
    """
    image_url: ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ URL (ì˜ˆ: '//img.danawa.com/...' í˜•ì‹)
    save_dir: ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ
    filename: ì €ì¥í•  íŒŒì¼ëª… (Noneì´ë©´ URLì—ì„œ ì¶”ì¶œ)
    """
    # URLì´ í”„ë¡œí† ì½œ ì—†ì´ // ë¡œ ì‹œì‘í•˜ë©´ https: ì¶”ê°€
    if image_url.startswith('//'):
        image_url = 'https:' + image_url
    
    # ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(save_dir, exist_ok=True)
    
    # filenameì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œê±°)
    if not filename:
        filename = os.path.basename(image_url.split('?')[0])
    save_path = os.path.join(save_dir, filename)
    
    try:
        response = requests.get(image_url, stream=True)
        response.raise_for_status()  # ìƒíƒœ ì½”ë“œ ì²´í¬
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        print(f"ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {image_url} -> {save_path}")
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url} ì—ëŸ¬: {e}")







'''






# HDF5 íŒŒì¼ ì—´ê¸° (ì½ê¸° ëª¨ë“œ)
with h5py.File(h5_filename, "r") as h5f:
    data_group = h5f["mixed_data"]

    for key in data_group.keys():
        item_group = data_group[key]
        print(f"ğŸ“¦ ë°ì´í„° ê·¸ë£¹: {key}")

        # ì´ë¦„ ë¶ˆëŸ¬ì˜¤ê¸°
        name = item_group.attrs["name"]
        print(f" - ì´ë¦„: {name}")

        # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
        if "image" in item_group:
            img_data = np.array(item_group["image"])
            print(f" - ì´ë¯¸ì§€ ë°ì´í„° í¬ê¸°: {img_data.shape}")

        # ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
        for attr_key, attr_value in item_group.attrs.items():
            if attr_key != "name":
                print(f" - ë©”íƒ€ë°ì´í„° [{attr_key}]: {attr_value}")

        # ë°°ì—´ ë¶ˆëŸ¬ì˜¤ê¸°
        if "array" in item_group:
            array_data = np.array(item_group["array"])
            print(f" - ë°°ì—´ ë°ì´í„°:\n{array_data}")

print("âœ… ë³µí•© ë¦¬ìŠ¤íŠ¸ HDF5 ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!")


'''





def extract_name(data,fname="output/danawa.csv"):
    out=[]
    for d in data:
        if not (d[0]=='get fail'):
            for i in d:
                if isinstance(i,bytes):
                    R=ast.literal_eval(i.decode("utf-8")) 
                    out.append(R["name"])

    out=pd.DataFrame(out)
    out.to_csv(fname)
    return out

def get_data_from_url_loop(url,start,end,clean_itam,product_lists,save_dir_image,limmiter,reviewfactor):
    print(clean_itam.blacklist)
    futures=range(start,end+1)
    fail=0
    for future in tqdm.tqdm(futures, total=end - start + 1):
        page = future
        try:
            data,fail=get_data_from_url_single(url,page,save_dir_image,fail,limmiter,reviewfactor) 
            product_lists.append(data)
            if fail>3:
                break
        except Exception as e:
            data = ["get fail"] 
            product_lists.append(data)
    return product_lists

def detect_company(name):
    """ì œí’ˆëª…ì—ì„œ ì œì¡°ì‚¬ ê°ì§€ (ê°„ë‹¨íˆ 'ì‚¼ì„±' ë˜ëŠ” 'ì• í”Œ/APPLE' í¬í•¨ ì—¬ë¶€ë¡œ ë¶„ë¥˜)"""
    if "ì‚¼ì„±" in name:
        return "ì‚¼ì„±"
    elif "ì• í”Œ" in name or "APPLE" in name:
        return "APPLE"
    else:
        return ""

def flatten_reviews(reviews):
    """
    ë¦¬ë·° í•­ëª©ì´ ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° í‰íƒ„í™”í•˜ì—¬ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜  
    (ì˜ê²¬ í…Œì´ë¸”ì— ì €ì¥í•  ë•Œ ì‚¬ìš©)
    """
    flat = []
    if isinstance(reviews, list):
        for item in reviews:
            flat.extend(flatten_reviews(item))
    else:
        flat.append(reviews)
    return flat

def export_custom_csv(pickle_file, output_dir):
    """
    pickle_fileì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ì½ì–´ ì•„ë˜ ì„¸ CSV íŒŒì¼ë¡œ ë¶„ë¦¬ ì €ì¥:
      1. product_table.csv: 
         - ì»¬ëŸ¼: ìƒí’ˆëª…, ê°€ê²©1, ê°€ê²©2, â€¦, ì´ë¯¸ì§€src, ì œì¡°ì‚¬,  
         - ê° ê°€ê²©ì€ "ê¸ˆì•¡ (ë©”ëª¨ë¦¬ sect)" í˜•ì‹
         - í‰ê· ë³„ì ëŠ” product['opinion']['reviews']ì—ì„œ ê¸¸ì´ 2ì¸ ë¦¬ìŠ¤íŠ¸ì˜ ë‘ ë²ˆì§¸ ìš”ì†Œë“¤ì„
           ëª¨ì•„ í‰ê· ì„ ê³„ì‚° (ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ í‰ê· , ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
      2. specs_table.csv:
         - ì»¬ëŸ¼: ì œí’ˆëª…, ìŠ¤í™1, ìŠ¤í™2, â€¦  
      3. opinions_table.csv:
         - ì»¬ëŸ¼: ì œí’ˆëª…, ì˜ê²¬, star  
         - ë³„ì (star)ì€ ë¦¬ë·° í•­ëª©ì´ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ë‘ ë²ˆì§¸ ìš”ì†Œ, ë¬¸ìì—´ì¸ ê²½ìš°ì—ëŠ” ë¹ˆ ë¬¸ìì—´
    """
    # í”¼í´ íŒŒì¼ ë¡œë“œ
    with open(pickle_file, "rb") as f:
        data = pickle.load(f)
    
    # ê° CSVì— ì €ì¥í•  í–‰ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    product_rows = []   # product_table.csvìš©
    specs_rows = []     # specs_table.csvìš©
    opinions_rows = []  # opinions_table.csvìš©

    # dataëŠ” ì—¬ëŸ¬ í˜ì´ì§€(ë¦¬ìŠ¤íŠ¸)ì˜ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
    all_products = []
    for page in data:
        all_products.extend(page)
    
    # ì œí’ˆë³„ ìµœëŒ€ ê°€ê²© ê°œìˆ˜ë¥¼ í™•ì¸ (ê°€ê²© ì •ë³´ë¥¼ CSVì˜ ì»¬ëŸ¼ ìˆ˜ë¡œ ì‚¬ìš©)
    max_price_count = 0
    for product in all_products:
        price_list = product.get("price", [])
        if len(price_list) > max_price_count:
            max_price_count = len(price_list)
    
    # ê°€ê²© ì»¬ëŸ¼ëª… ìƒì„± (ì˜ˆ: ê°€ê²©1, ê°€ê²©2, â€¦)
    price_columns = [f"ê°€ê²©{i+1}" for i in range(max_price_count)]
    
    # ê° ì œí’ˆë³„ë¡œ ì²˜ë¦¬
    for product in all_products:
        name = product.get("name", "")
        # 1. product_table.csvìš© ë°ì´í„° ìƒì„±
        price_list = product.get("price", [])
        prices_formatted = []
        for price_item in price_list:
            p_val = price_item.get("price", "")
            mem = price_item.get("memory_sect", "")
            prices_formatted.append(f"{p_val} ({mem})")
        # ë¹ˆì¹¸ìœ¼ë¡œ ì±„ì›€
        while len(prices_formatted) < max_price_count:
            prices_formatted.append("")
        
        image_src = product.get("image", {}).get("src", "")
        manufacturer = detect_company(name)
        
        # í‰ê· ë³„ì  ê³„ì‚°: opinion['reviews']ì—ì„œ ê¸¸ì´ 2ì¸ ë¦¬ìŠ¤íŠ¸ í•­ëª©ì˜ ë‘ë²ˆì§¸ ìš”ì†Œë¥¼ ë³„ì ìœ¼ë¡œ ì·¨ê¸‰
        opinion_data = product.get("opinion", {})
        reviews = opinion_data.get("reviews", [])
        stars = []
        for review in reviews:
            if isinstance(review, list) and len(review) == 2:
                star_str = review[1]
                # ë³„ì  ë¬¸ìì—´ì—ì„œ ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì˜ˆ: "100ì " -> "100")
                nums = re.findall(r'\d+\.?\d*', star_str)
                if nums:
                    try:
                        star_val = float(nums[0])
                        stars.append(star_val)
                    except:
                        pass
        if stars:
            average_star = sum(stars) / len(stars)
            average_star = round(average_star, 2)
        else:
            average_star = ""
        
        prod_row = {"ìƒí’ˆëª…": name}
        for idx, col in enumerate(price_columns):
            prod_row[col] = prices_formatted[idx]
        prod_row["ì´ë¯¸ì§€src"] = image_src
        prod_row["ì œì¡°ì‚¬"] = manufacturer
        prod_row["í‰ê· ë³„ì "] = average_star
        product_rows.append(prod_row)
        
        # 2. specs_table.csvìš© ë°ì´í„° ìƒì„±
        specs = product.get("specs", [])
        spec_row = {"ì œí’ˆëª…": name}
        for i, spec in enumerate(specs, start=1):
            spec_row[f"ìŠ¤í™{i}"] = spec
        specs_rows.append(spec_row)
        
        # 3. opinions_table.csvìš© ë°ì´í„° ìƒì„±
        # opinion['reviews']ì—ì„œ ë¦¬ë·°ê°€ ë¦¬ìŠ¤íŠ¸ì´ë©´, ë¦¬ë·°[0]ì„ ì˜ê²¬, ë¦¬ë·°[1]ì„ starë¡œ ì €ì¥;
        # ë¬¸ìì—´ ë¦¬ë·°ëŠ” starëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        for review in reviews:
            if isinstance(review, list) and len(review) == 2:
                opinions_rows.append({
                    "ì œí’ˆëª…": name,
                    "ì˜ê²¬": review[0],
                    "star": review[1]
                })
            elif isinstance(review, str):
                opinions_rows.append({
                    "ì œí’ˆëª…": name,
                    "ì˜ê²¬": review,
                    "star": ""
                })
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. product_table.csv ì €ì¥
    product_csv_path = os.path.join(output_dir, "product_table.csv")
    product_fieldnames = ["ìƒí’ˆëª…"] + price_columns + ["ì´ë¯¸ì§€src", "ì œì¡°ì‚¬", "í‰ê· ë³„ì "]
    with open(product_csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=product_fieldnames)
        writer.writeheader()
        writer.writerows(product_rows)
    
    # 2. specs_table.csv ì €ì¥
    max_spec_count = 0
    for row in specs_rows:
        count = sum(1 for key in row if key.startswith("ìŠ¤í™"))
        if count > max_spec_count:
            max_spec_count = count
    specs_fieldnames = ["ì œí’ˆëª…"] + [f"ìŠ¤í™{i}" for i in range(1, max_spec_count+1)]
    specs_csv_path = os.path.join(output_dir, "specs_table.csv")
    with open(specs_csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=specs_fieldnames)
        writer.writeheader()
        for row in specs_rows:
            for i in range(1, max_spec_count+1):
                key = f"ìŠ¤í™{i}"
                if key not in row:
                    row[key] = ""
            writer.writerow(row)
    
    # 3. opinions_table.csv ì €ì¥
    opinions_csv_path = os.path.join(output_dir, "opinions_table.csv")
    opinions_fieldnames = ["ì œí’ˆëª…", "ì˜ê²¬", "star"]
    with open(opinions_csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=opinions_fieldnames)
        writer.writeheader()
        writer.writerows(opinions_rows)
    
    print("CSV íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:", output_dir)




if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='danawa_crawler')
    parser.add_argument('--url', type=str, default="https://prod.danawa.com/list/?cate=22254631", help='url')
    parser.add_argument('--start', type=int, default=1, help='start page')
    parser.add_argument('--end', type=int, default=11, help='end page')
    parser.add_argument('--output', type=str, default="output", help='output directory')
    parser.add_argument('--csv_path', type=str, default="output/csv", help='save csv path')
    parser.add_argument('--pickle_path', type=str, default="output/pickle", help='save pickle path')
    parser.add_argument('--image_path', type=str, default="output/images", help='save image path')
    parser.add_argument('--limmiter', type=int, default="100", help='limit for debug')
    parser.add_argument('--reviewfactor', type=bool, default=True, help='review factor for debug')
    args = parser.parse_args()
    url = args.url
    start = args.start
    end = args.end
    output = args.output
    csv_path = args.csv_path
    pickle_path = args.pickle_path
    image_path = args.image_path
    limmiter=args.limmiter
    reviewfactor=args.reviewfactor
    # ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” '/'ì™€ null ë¬¸ìê°€ ë¬¸ì œë˜ì§€ë§Œ, '/'ëŠ” ë°˜ë“œì‹œ ì¹˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì— Windowsì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ìë“¤ë„ í•¨ê»˜ ì¹˜í™˜í•˜ë©´ í˜¸í™˜ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.
    safe_url = re.sub(r'[\\/:*?"<>|]', '_', url)
    hashed_filename = hashlib.sha256(url.encode("utf-8")).hexdigest()
    csvname=hashed_filename+".csv"
    # ì•ˆì „í•œ ê²½ë¡œ ìƒì„±
    save_dir_image = Path(image_path) / safe_url
    save_dir_image.mkdir(parents=True, exist_ok=True)
    print(f"ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ: {save_dir_image}")
    print(f"í˜„ì¬ ì›Œí‚¹ ë””ë ‰í† ë¦¬: {os.getcwd()}")
    csv_filename=csv_path+"/"+csvname
    csv_raw_filename=csv_path+"/"+hashed_filename+"_raw.csv"
    pickle_filename=pickle_path+"/"+hashed_filename+".pickle"
    pickle_output=pickle_path+"/"+hashed_filename+"_output"+".pickle"
    csv_filpath=Path(csv_filename)
    csv_filpath.touch(exist_ok=True)  
    file_path = Path('output/flag.txt')
    file_path.touch()
    with open('output/flag.txt', 'w') as f:
        f.write("1")
    clean_itam=ProductDatabasePickleFixed(pickle_filename=pickle_filename, csv_filename=csv_filename)
    
    product_list=[]
    
    data =get_data_from_url_loop(url,1,1000,clean_itam,product_list,save_dir_image,limmiter,reviewfactor)
    clean_itam.export_to_csv()
    clean_itam.export_raw_data_to_csv(csv_raw_filename)
    extract_name(data)
    with open(pickle_output, "wb") as f:
        pickle.dump(data, f) 
    export_custom_csv(pickle_output, csv_path)

    
